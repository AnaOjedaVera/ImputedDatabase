{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bc3d60-365e-4cd8-926c-582746e5f05d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0183f6-26f7-406a-996d-b1d6f128faa4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4712c0ea-f725-469b-aee5-a9820c4b2fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'var_smoothing': 1e-12}\n",
      "Best cross-validation score: 0.6701 ± 0.0134\n",
      "Test Accuracy: 0.6599 (95% CI: 0.6216 - 0.6982)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.32      1.00      0.49        11\n",
      "           B       0.45      0.83      0.59        12\n",
      "           C       0.75      0.71      0.73        58\n",
      "          Ch       0.50      0.36      0.42        33\n",
      "           D       0.73      0.86      0.79        22\n",
      "           E       0.41      0.92      0.57        13\n",
      "           K       0.19      0.54      0.29        13\n",
      "           L       0.38      0.71      0.50        14\n",
      "           M       0.62      0.55      0.58        55\n",
      "           P       0.69      0.31      0.42        36\n",
      "           Q       0.56      0.90      0.69        30\n",
      "           R       0.00      0.00      0.00         3\n",
      "           S       0.97      0.66      0.78       240\n",
      "           V       0.97      0.88      0.92        41\n",
      "           Z       0.57      0.57      0.57         7\n",
      "\n",
      "    accuracy                           0.66       588\n",
      "   macro avg       0.54      0.65      0.56       588\n",
      "weighted avg       0.77      0.66      0.68       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-GNB-A1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune the variance smoothing parameter of GaussianNB.\n",
    "#    Values range logarithmically from 1e-10 to 1e-6.\n",
    "param_grid = {'var_smoothing': np.logspace(-12, -6, num=2)}\n",
    "gnb = GaussianNB()\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "grid_search = GridSearchCV(gnb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final GaussianNB model with the best parameter.\n",
    "best_gnb = GaussianNB(**best_params)\n",
    "best_gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_gnb.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_gnb, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_gnb.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_gnb.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-GNB-A1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters (variance smoothing):\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    # Create a table that shows only the Mean and Std for each metric.\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time  \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5158b-ced6-4229-ae7d-ffe3a3c3969b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe7458d-2ade-4fc1-b164-84e34916a1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'var_smoothing': 1e-12}\n",
      "Best cross-validation score: 0.6378 ± 0.0134\n",
      "Test Accuracy: 0.6310 (95% CI: 0.5919 - 0.6700)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       0.32      1.00      0.49        11\n",
      "           B       0.45      0.83      0.59        12\n",
      "           C       0.73      0.71      0.72        58\n",
      "          Ch       0.52      0.36      0.43        33\n",
      "           D       0.70      0.86      0.78        22\n",
      "           E       0.21      0.54      0.30        13\n",
      "           K       0.16      0.46      0.24        13\n",
      "           L       0.37      0.71      0.49        14\n",
      "           M       0.58      0.47      0.52        55\n",
      "           P       0.36      0.14      0.20        36\n",
      "           Q       0.56      0.90      0.69        30\n",
      "           R       0.00      0.00      0.00         3\n",
      "           S       0.97      0.65      0.78       240\n",
      "           V       0.97      0.88      0.92        41\n",
      "           Z       0.57      0.57      0.57         7\n",
      "\n",
      "    accuracy                           0.63       588\n",
      "   macro avg       0.50      0.61      0.51       588\n",
      "weighted avg       0.74      0.63      0.66       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-GNB1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune the variance smoothing parameter of GaussianNB.\n",
    "#    Values range logarithmically from 1e-10 to 1e-6.\n",
    "param_grid = {'var_smoothing': np.logspace(-12, -6, num=2)}\n",
    "gnb = GaussianNB()\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "grid_search = GridSearchCV(gnb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final GaussianNB model with the best parameter.\n",
    "best_gnb = GaussianNB(**best_params)\n",
    "best_gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_gnb.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_gnb, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_gnb.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_gnb.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-GNB1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters (variance smoothing):\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    # Create a table that shows only the Mean and Std for each metric.\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time  \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607f3fb-26b9-472e-922a-fad900279eb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ed186-6448-4675-9e41-ad46eeeb2ca2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab0ffde-c072-48fe-bb2a-46020f5466ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'hidden_layer_sizes': (64, 64), 'learning_rate_init': 0.05, 'max_iter': 1000, 'solver': 'sgd'}\n",
      "Best cross-validation score: 0.8861 ± 0.0097\n",
      "Test Accuracy: 0.8690 (95% CI: 0.8418 - 0.8963)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.91      0.95        11\n",
      "           B       0.67      0.83      0.74        12\n",
      "           C       0.85      0.88      0.86        58\n",
      "          Ch       0.76      0.76      0.76        33\n",
      "           D       0.79      0.86      0.83        22\n",
      "           E       0.81      1.00      0.90        13\n",
      "           K       0.91      0.77      0.83        13\n",
      "           L       0.65      0.79      0.71        14\n",
      "           M       0.90      0.64      0.74        55\n",
      "           P       0.74      0.72      0.73        36\n",
      "           Q       0.93      0.83      0.88        30\n",
      "           R       0.00      0.00      0.00         3\n",
      "           S       0.92      0.96      0.94       240\n",
      "           V       0.98      0.98      0.98        41\n",
      "           Z       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.87       588\n",
      "   macro avg       0.79      0.79      0.78       588\n",
      "weighted avg       0.87      0.87      0.87       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-MLP-A1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Multilayer Perceptron.\n",
    "# Hyperparameter grid: hidden_layer_sizes, learning_rate_init, solver, max_iter.\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(32, 32), (64, 64), (32, 32, 32), (64, 64, 64)],\n",
    "    'learning_rate_init': [0.01, 0.05, 0.1],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'max_iter': [1000, 2500, 5000]\n",
    "}\n",
    "mlp = MLPClassifier(random_state=42, activation='relu')\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Multilayer Perceptron model with the best parameters.\n",
    "best_mlp = MLPClassifier(random_state=42, **best_params, activation='relu', tol=1e-3)\n",
    "best_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_mlp.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_mlp, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_mlp.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_mlp.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLP-A1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4003c-7f8e-452e-a7c0-87d7fb3d52b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845e135-951d-4409-9510-1ea5cd02c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Multilayer Perceptron.\n",
    "# Hyperparameter grid: hidden_layer_sizes, learning_rate_init, solver, max_iter.\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(32, 32), (64, 64), (32, 32, 32), (64, 64, 64)],\n",
    "    'learning_rate_init': [0.01, 0.05, 0.1],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'max_iter': [1000, 2500, 5000]\n",
    "}\n",
    "mlp = MLPClassifier(random_state=42, activation='relu')\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Multilayer Perceptron model with the best parameters.\n",
    "best_mlp = MLPClassifier(random_state=42, **best_params, activation='relu', tol=1e-3)\n",
    "best_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_mlp.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_mlp, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by total number of samples to get percentages.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_mlp.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_mlp.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLP1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5886c3-812e-4de0-9337-35212f09eb28",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca87087-844e-4cee-8a32-76672d397f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f8828c-8da8-48d3-bd61-def37c0cb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'C': 5, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.8844 ± 0.0080\n",
      "Test Accuracy: 0.8673 (95% CI: 0.8399 - 0.8948)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.91      0.95        11\n",
      "           B       1.00      0.75      0.86        12\n",
      "           C       0.82      0.93      0.87        58\n",
      "          Ch       0.84      0.79      0.81        33\n",
      "           D       0.86      0.86      0.86        22\n",
      "           E       0.92      0.92      0.92        13\n",
      "           K       0.78      0.54      0.64        13\n",
      "           L       0.50      0.57      0.53        14\n",
      "           M       0.79      0.67      0.73        55\n",
      "           P       0.74      0.81      0.77        36\n",
      "           Q       0.75      0.80      0.77        30\n",
      "           R       1.00      0.33      0.50         3\n",
      "           S       0.93      0.95      0.94       240\n",
      "           V       0.98      0.98      0.98        41\n",
      "           Z       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.87       588\n",
      "   macro avg       0.84      0.78      0.80       588\n",
      "weighted avg       0.87      0.87      0.87       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-MLR-A1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for multinomial logistic regression.\n",
    "C_values = list(range(5, 61, 5)) + list(np.logspace(-4, 4, 20))\n",
    "param_grid = {\n",
    "    'C': C_values,\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [10000, 25000, 50000]\n",
    "}\n",
    "\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final multinomial logistic regression model with the best parameters.\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "best_logreg = LogisticRegression(random_state=42, **best_params)\n",
    "best_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_logreg.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_logreg, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_logreg.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_logreg.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLR-A1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73b72d-c310-4ee3-926d-93a88794f0ae",
   "metadata": {},
   "source": [
    "## Without Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36a5860-88ab-44f3-b47e-d4355eece406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'C': 4.281332398719396, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.8457 ± 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8452 (95% CI: 0.8160 - 0.8745)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.91      0.95        11\n",
      "           B       0.89      0.67      0.76        12\n",
      "           C       0.78      0.91      0.84        58\n",
      "          Ch       0.86      0.76      0.81        33\n",
      "           D       0.86      0.86      0.86        22\n",
      "           E       0.68      1.00      0.81        13\n",
      "           K       0.67      0.46      0.55        13\n",
      "           L       0.53      0.64      0.58        14\n",
      "           M       0.67      0.60      0.63        55\n",
      "           P       0.73      0.53      0.61        36\n",
      "           Q       0.75      0.80      0.77        30\n",
      "           R       0.00      0.00      0.00         3\n",
      "           S       0.93      0.96      0.94       240\n",
      "           V       0.95      0.98      0.96        41\n",
      "           Z       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.85       588\n",
      "   macro avg       0.75      0.74      0.74       588\n",
      "weighted avg       0.84      0.85      0.84       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-MLR1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for multinomial logistic regression.\n",
    "C_values = list(range(5, 61, 5)) + list(np.logspace(-4, 4, 20))\n",
    "param_grid = {\n",
    "    'C': C_values,\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [10000, 25000, 50000]\n",
    "}\n",
    "\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final multinomial logistic regression model with the best parameters.\n",
    "# Removed multi_class parameter and increased max_iter to 5000.\n",
    "best_logreg = LogisticRegression(random_state=42, **best_params)\n",
    "best_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_logreg.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_logreg, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_logreg.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_logreg.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-MLR1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59259d2-58b6-4227-8669-979e5031df77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cdbfa-33b8-4b16-804e-4510a0d582a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe632c3-3f15-4baf-ab87-b4b6acb47e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 10, 'min_samples_split': 15, 'n_estimators': 50}\n",
      "Best cross-validation score: 0.8571 ± 0.0061\n",
      "Test Accuracy: 0.8350 (95% CI: 0.8050 - 0.8650)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.82      0.90        11\n",
      "           B       0.69      0.75      0.72        12\n",
      "           C       0.79      0.90      0.84        58\n",
      "          Ch       0.83      0.61      0.70        33\n",
      "           D       0.67      0.82      0.73        22\n",
      "           E       1.00      0.92      0.96        13\n",
      "           K       0.71      0.38      0.50        13\n",
      "           L       0.57      0.29      0.38        14\n",
      "           M       0.79      0.69      0.74        55\n",
      "           P       0.74      0.78      0.76        36\n",
      "           Q       0.77      0.90      0.83        30\n",
      "           R       0.00      0.00      0.00         3\n",
      "           S       0.88      0.97      0.92       240\n",
      "           V       1.00      0.90      0.95        41\n",
      "           Z       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.84       588\n",
      "   macro avg       0.70      0.65      0.66       588\n",
      "weighted avg       0.82      0.84      0.82       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-RF-A1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Random Forest.\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9, 15],  # Tree depth\n",
    "    'min_samples_split': [15, 20, 30, 40],  # Minimum samples to split\n",
    "    'min_samples_leaf': [10, 15, 20, 30],  # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2', 0.5, None]  # Feature selection method\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Random Forest model with the best parameters.\n",
    "best_rf = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_rf, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_rf.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_rf.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-RF-A1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93664e65-8cd0-47ec-bcdb-12dec2db6d52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f156334-3d14-40c1-9464-fb7cfaa57fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 15, 'n_estimators': 50}\n",
      "Best cross-validation score: 0.8112 ± 0.0086\n",
      "Test Accuracy: 0.8010 (95% CI: 0.7688 - 0.8333)\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.82      0.90        11\n",
      "           B       0.69      0.75      0.72        12\n",
      "           C       0.79      0.86      0.83        58\n",
      "          Ch       0.84      0.64      0.72        33\n",
      "           D       0.70      0.73      0.71        22\n",
      "           E       0.75      0.46      0.57        13\n",
      "           K       0.40      0.31      0.35        13\n",
      "           L       0.50      0.29      0.36        14\n",
      "           M       0.62      0.67      0.64        55\n",
      "           P       0.62      0.42      0.50        36\n",
      "           Q       0.76      0.83      0.79        30\n",
      "           R       0.00      0.00      0.00         3\n",
      "           S       0.87      0.97      0.92       240\n",
      "           V       1.00      0.98      0.99        41\n",
      "           Z       0.60      0.43      0.50         7\n",
      "\n",
      "    accuracy                           0.80       588\n",
      "   macro avg       0.68      0.61      0.63       588\n",
      "weighted avg       0.79      0.80      0.79       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-RF1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for Random Forest.\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9, 15],  # Tree depth\n",
    "    'min_samples_split': [15, 20, 30, 40],  # Minimum samples to split\n",
    "    'min_samples_leaf': [10, 15, 20, 30],  # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2', 0.5, None]  # Feature selection method\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final Random Forest model with the best parameters.\n",
    "best_rf = RandomForestClassifier(random_state=42, **best_params)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_rf, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_rf.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_rf.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-RF1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time and Total Models Trained.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85703650-519c-4ce8-9cdf-89c7b5c3f1fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dcd39c-579f-41b5-bdf9-aeb88e818efa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## With Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc3094d-0bd2-4997-9bf7-cacc48245bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'C': 19, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.8929 ± 0.0043\n",
      "Test Accuracy: 0.8810 (95% CI: 0.8548 - 0.9071)\n",
      "Balanced Accuracy: 0.8007\n",
      "F1 Score (weighted): 0.8794\n",
      "MCC: 0.8507\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.91      0.95        11\n",
      "           B       1.00      0.67      0.80        12\n",
      "           C       0.82      0.95      0.88        58\n",
      "          Ch       0.83      0.76      0.79        33\n",
      "           D       0.83      0.91      0.87        22\n",
      "           E       0.87      1.00      0.93        13\n",
      "           K       0.77      0.77      0.77        13\n",
      "           L       0.62      0.71      0.67        14\n",
      "           M       0.84      0.69      0.76        55\n",
      "           P       0.81      0.81      0.81        36\n",
      "           Q       0.74      0.87      0.80        30\n",
      "           R       1.00      0.33      0.50         3\n",
      "           S       0.94      0.95      0.95       240\n",
      "           V       1.00      0.98      0.99        41\n",
      "           Z       0.83      0.71      0.77         7\n",
      "\n",
      "    accuracy                           0.88       588\n",
      "   macro avg       0.86      0.80      0.82       588\n",
      "weighted avg       0.88      0.88      0.88       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-SVM-A1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns + ['pV']]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for SVM.\n",
    "# Hyperparameter grid: kernel (linear or rbf), regularization parameter 'C' (6 to 24),\n",
    "# and for the RBF kernel, gamma as 'scale' or 'auto'.\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': list(range(6, 25))},\n",
    "    {'kernel': ['rbf'], 'C': list(range(6, 25)), 'gamma': ['scale', 'auto']}\n",
    "]\n",
    "svc = SVC(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final SVM model with the best parameters.\n",
    "best_svm = SVC(random_state=42, **best_params)\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "test_mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Balanced Accuracy: {:.4f}\".format(test_balanced_accuracy))\n",
    "print(\"F1 Score (weighted): {:.4f}\".format(test_f1))\n",
    "print(\"MCC: {:.4f}\".format(test_mcc))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_svm, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_svm.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_svm.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-SVM-A1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Balanced Accuracy: {test_balanced_accuracy:.4f}\n",
    "F1 Score (weighted): {test_f1:.4f}\n",
    "MCC: {test_mcc:.4f}\n",
    "\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35378b53-b130-4081-aa33-31064a9922e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Without Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7726f5a7-a389-4a9f-b050-26e6726a8731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV: {'C': 29.763514416313132, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.8674 ± 0.0109\n",
      "Test Accuracy: 0.8537 (95% CI: 0.8252 - 0.8823)\n",
      "Balanced Accuracy: 0.7648\n",
      "F1 Score (weighted): 0.8517\n",
      "MCC: 0.8162\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.91      0.95        11\n",
      "           B       1.00      0.67      0.80        12\n",
      "           C       0.78      0.88      0.83        58\n",
      "          Ch       0.78      0.76      0.77        33\n",
      "           D       0.86      0.86      0.86        22\n",
      "           E       0.76      1.00      0.87        13\n",
      "           K       0.88      0.54      0.67        13\n",
      "           L       0.50      0.71      0.59        14\n",
      "           M       0.74      0.64      0.69        55\n",
      "           P       0.80      0.67      0.73        36\n",
      "           Q       0.72      0.87      0.79        30\n",
      "           R       1.00      0.33      0.50         3\n",
      "           S       0.93      0.95      0.94       240\n",
      "           V       0.98      0.98      0.98        41\n",
      "           Z       0.83      0.71      0.77         7\n",
      "\n",
      "    accuracy                           0.85       588\n",
      "   macro avg       0.84      0.76      0.78       588\n",
      "weighted avg       0.86      0.85      0.85       588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [4] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\anaoj\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report has been saved as '03-SVM1.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import time  # Added to measure processing time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# ----------------------------\n",
    "# Set global font settings\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "# ----------------------------\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()  # Record the start time of the process\n",
    "\n",
    "# 1. Load the merged database and prepare features/target.\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df = df.drop(columns=['counts', 'class_bdm'])\n",
    "\n",
    "# Exclude 'pV' (albedo) and 'name'; target column is 'class_asteroid_sf'\n",
    "spectra_columns = [col for col in df.columns if col not in ['pV', 'name', 'class_asteroid_sf']]\n",
    "X = df[spectra_columns]\n",
    "y = df['class_asteroid_sf']\n",
    "names = df['name']\n",
    "\n",
    "# 2. Split data into training (80%) and test (20%) sets (stratified by target)\n",
    "X_train, X_test, y_train, y_test, name_train, name_test = train_test_split(\n",
    "    X, y, names, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize features (fit on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Tune hyperparameters for SVM.\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': list(range(6, 25)) + list(np.logspace(-4, 4, 20))},\n",
    "    {'kernel': ['rbf'], 'C': list(range(6, 25)) + list(np.logspace(-4, 4, 20)), 'gamma': ['scale', 'auto']}\n",
    "]\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "# Use all CPUs by setting n_jobs=-1 in GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "# Retrieve the CV standard deviation for the best parameter:\n",
    "idx = np.where(grid_search.cv_results_['mean_test_score'] == best_cv_score)[0][0]\n",
    "cv_std = grid_search.cv_results_['std_test_score'][idx]\n",
    "print(\"Best parameters from GridSearchCV:\", best_params)\n",
    "print(\"Best cross-validation score: {:.4f} ± {:.4f}\".format(best_cv_score, cv_std))\n",
    "\n",
    "# 5. Train final SVM model with the best parameters.\n",
    "best_svm = SVC(random_state=42, **best_params)\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "test_mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Compute 95% confidence interval for accuracy using the binomial approximation:\n",
    "n_test = len(y_test)\n",
    "se = math.sqrt(test_accuracy * (1 - test_accuracy) / n_test)\n",
    "ci_lower = test_accuracy - 1.96 * se\n",
    "ci_upper = test_accuracy + 1.96 * se\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.4f} (95% CI: {:.4f} - {:.4f})\".format(test_accuracy, ci_lower, ci_upper))\n",
    "print(\"Balanced Accuracy: {:.4f}\".format(test_balanced_accuracy))\n",
    "print(\"F1 Score (weighted): {:.4f}\".format(test_f1))\n",
    "print(\"MCC: {:.4f}\".format(test_mcc))\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 7. Generate learning curves (with error bands).\n",
    "# Use all CPUs by setting n_jobs=-1\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_svm, X_train_scaled, y_train, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# 8. Compute a proxy for feature importance using ANOVA F-statistic.\n",
    "f_values, p_values = f_classif(X_train_scaled, y_train)\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'f_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='f_value', ascending=False)\n",
    "\n",
    "# 9. Identify misclassified examples (including asteroid names).\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'name': name_test[y_test != y_pred],\n",
    "    'true_label': y_test[y_test != y_pred],\n",
    "    'predicted_label': pd.Series(y_pred, index=y_test.index)[y_test != y_pred]\n",
    "})\n",
    "\n",
    "# 10. Bootstrap function to compute confusion matrix percentages.\n",
    "def bootstrap_confusion_matrix(y_true, y_pred, iterations=1000):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    n = len(y_true)\n",
    "    cm_list = []\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        cm = confusion_matrix(y_true.iloc[indices],\n",
    "                              pd.Series(y_pred, index=y_true.index).iloc[indices],\n",
    "                              labels=unique_labels)\n",
    "        # Normalize by row sums so that each actual label sums to 100%\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # avoid division by zero\n",
    "        cm_percent = (cm / row_sums) * 100\n",
    "        cm_list.append(cm_percent)\n",
    "    cm_array = np.stack(cm_list, axis=0)\n",
    "    cm_mean = np.mean(cm_array, axis=0)\n",
    "    return unique_labels, cm_mean\n",
    "\n",
    "unique_labels, cm_mean = bootstrap_confusion_matrix(y_test, y_pred, iterations=1000)\n",
    "\n",
    "# 11. Perform 10-run cross validation on the training set to compute aggregated metrics.\n",
    "n_runs = 10\n",
    "metrics_list = []\n",
    "for run in range(n_runs):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=run)\n",
    "    run_acc, run_bacc, run_f1, run_mcc = [], [], [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "        X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        best_svm.fit(X_cv_train, y_cv_train)\n",
    "        y_cv_pred = best_svm.predict(X_cv_val)\n",
    "        run_acc.append(accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_bacc.append(balanced_accuracy_score(y_cv_val, y_cv_pred))\n",
    "        run_f1.append(f1_score(y_cv_val, y_cv_pred, average='weighted'))\n",
    "        run_mcc.append(matthews_corrcoef(y_cv_val, y_cv_pred))\n",
    "    metrics_list.append({\n",
    "        'Accuracy': np.mean(run_acc),\n",
    "        'Balanced Accuracy': np.mean(run_bacc),\n",
    "        'F1': np.mean(run_f1),\n",
    "        'MCC': np.mean(run_mcc)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(metrics_list)\n",
    "agg_metrics = df_cv.agg(['mean', 'std']).T.round(4)\n",
    "\n",
    "# 12. Create a multi-page PDF report.\n",
    "pdf_filename = \"03-SVM1.pdf\"\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # Page 1: Summary of results and best parameters.\n",
    "    fig1 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"Model Evaluation Summary\n",
    "\n",
    "Test Accuracy: {test_accuracy:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\n",
    "Balanced Accuracy: {test_balanced_accuracy:.4f}\n",
    "F1 Score (weighted): {test_f1:.4f}\n",
    "MCC: {test_mcc:.4f}\n",
    "\n",
    "Best Cross-Validation Score: {best_cv_score:.4f} ± {cv_std:.4f}\n",
    "\n",
    "Best Parameters:\n",
    "{best_params}\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\"\"\"\n",
    "    plt.text(0.05, 0.95, summary_text, verticalalignment='top', wrap=True)\n",
    "    pdf.savefig(fig1)\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # Page 2: Confusion Matrix in percentages.\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 16))\n",
    "    annot = np.empty_like(cm_mean).astype(str)\n",
    "    for i in range(cm_mean.shape[0]):\n",
    "        for j in range(cm_mean.shape[1]):\n",
    "            annot[i, j] = f\"{cm_mean[i, j]:.1f}%\"\n",
    "    sns.heatmap(cm_mean, annot=annot, fmt=\"\", cmap=\"Blues\", ax=ax2,\n",
    "                xticklabels=unique_labels, yticklabels=unique_labels,\n",
    "                annot_kws={\"size\": 17}, vmin=0, vmax=100,\n",
    "                cbar_kws={'ticks': np.linspace(0, 100, 11), 'format': '%.0f%%'})\n",
    "    ax2.set_title(\"Confusion Matrix (in %)\", fontsize=28)\n",
    "    ax2.set_xlabel(\"Predicted Label\", fontsize=24)\n",
    "    ax2.set_ylabel(\"True Label\", fontsize=24)\n",
    "    pdf.savefig(fig2)\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Page 3: Learning Curves with error bands.\n",
    "    fig3 = plt.figure(figsize=(16, 12))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', label=\"Validation Score\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.2)\n",
    "    plt.title(\"Learning Curves\", fontsize=28)\n",
    "    plt.xlabel(\"Number of Training Examples\", fontsize=24)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "    plt.legend(loc=\"best\", fontsize=20)\n",
    "    pdf.savefig(fig3)\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Page 4: Feature Importances (F-statistic proxy).\n",
    "    fig4 = plt.figure(figsize=(16, 12))\n",
    "    plt.bar(feature_importances_df['feature'], feature_importances_df['f_value'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Feature Importances (F-statistic)\", fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig4)\n",
    "    plt.close(fig4)\n",
    "    \n",
    "    # Page 5: Table of Misclassified Examples.\n",
    "    fig5, ax5 = plt.subplots(figsize=(16, 12))\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    table = ax5.table(cellText=misclassified_df.values,\n",
    "                      colLabels=misclassified_df.columns,\n",
    "                      loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax5.set_title(\"Misclassified Examples\", fontsize=28)\n",
    "    pdf.savefig(fig5)\n",
    "    plt.close(fig5)\n",
    "    \n",
    "    # Page 6: Aggregated Cross Validation Metrics.\n",
    "    fig6, ax6 = plt.subplots(figsize=(18, 10))\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    table = ax6.table(cellText=agg_metrics.values,\n",
    "                      rowLabels=agg_metrics.index,\n",
    "                      colLabels=agg_metrics.columns,\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 0.8, 0.3])  # Adjust these values as needed\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1.2, 1.2)\n",
    "    ax6.set_title(\"Cross Validation Metrics (10 Runs)\\nMean and Standard Deviation\", fontsize=28)\n",
    "    pdf.savefig(fig6)\n",
    "    plt.close(fig6)\n",
    "    \n",
    "    # Page 7: Total Processing Time.\n",
    "    elapsed_time = time.time() - start_time  # Compute the total elapsed time\n",
    "      \n",
    "    fig7 = plt.figure(figsize=(16, 12))\n",
    "    plt.axis('off')\n",
    "    text = (f\"Total Processing Time: {elapsed_time:.2f} seconds\\n\\n\")\n",
    "    plt.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=28)\n",
    "    pdf.savefig(fig7)\n",
    "    plt.close(fig7)\n",
    "\n",
    "print(f\"PDF report has been saved as '{pdf_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160b39c-4ab3-4387-9251-cfe4926b9973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
