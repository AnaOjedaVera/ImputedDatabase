{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bc3d60-365e-4cd8-926c-582746e5f05d",
   "metadata": {},
   "source": [
    "# Imputation method of spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0183f6-26f7-406a-996d-b1d6f128faa4",
   "metadata": {},
   "source": [
    "## Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8f8524c7-2e32-4267-9668-045ac9eab58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.475</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.525</th>\n",
       "      <th>0.55</th>\n",
       "      <th>0.575</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.625</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.675</th>\n",
       "      <th>...</th>\n",
       "      <th>2.25</th>\n",
       "      <th>2.3</th>\n",
       "      <th>2.35</th>\n",
       "      <th>2.4</th>\n",
       "      <th>2.45</th>\n",
       "      <th>pV</th>\n",
       "      <th>name</th>\n",
       "      <th>counts</th>\n",
       "      <th>class_bdm</th>\n",
       "      <th>class_asteroid_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178308</td>\n",
       "      <td>0.179383</td>\n",
       "      <td>0.177941</td>\n",
       "      <td>0.173358</td>\n",
       "      <td>0.168305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1988 TA</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231438</td>\n",
       "      <td>0.232110</td>\n",
       "      <td>0.218402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.391474</td>\n",
       "      <td>1990 WO3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220376</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.222016</td>\n",
       "      <td>0.222024</td>\n",
       "      <td>0.240588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993 FO6</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154970</td>\n",
       "      <td>0.151323</td>\n",
       "      <td>0.144752</td>\n",
       "      <td>0.138640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.655608</td>\n",
       "      <td>1998 FE118</td>\n",
       "      <td>1</td>\n",
       "      <td>Sa</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210976</td>\n",
       "      <td>0.201246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.422508</td>\n",
       "      <td>1998 YG8</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320975</td>\n",
       "      <td>0.337816</td>\n",
       "      <td>0.349726</td>\n",
       "      <td>0.362664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.657577</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279271</td>\n",
       "      <td>0.297687</td>\n",
       "      <td>0.316016</td>\n",
       "      <td>0.332846</td>\n",
       "      <td>0.346252</td>\n",
       "      <td>-0.958607</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.760226</td>\n",
       "      <td>-0.727909</td>\n",
       "      <td>-0.699084</td>\n",
       "      <td>-0.676065</td>\n",
       "      <td>-0.651195</td>\n",
       "      <td>-0.622867</td>\n",
       "      <td>-0.595565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217393</td>\n",
       "      <td>0.225075</td>\n",
       "      <td>0.236595</td>\n",
       "      <td>0.251763</td>\n",
       "      <td>0.275564</td>\n",
       "      <td>-1.173925</td>\n",
       "      <td>Thestor</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>-0.424504</td>\n",
       "      <td>-0.432160</td>\n",
       "      <td>-0.434251</td>\n",
       "      <td>-0.432117</td>\n",
       "      <td>-0.432182</td>\n",
       "      <td>-0.426685</td>\n",
       "      <td>-0.424460</td>\n",
       "      <td>-0.413358</td>\n",
       "      <td>-0.403189</td>\n",
       "      <td>-0.394357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240991</td>\n",
       "      <td>0.251120</td>\n",
       "      <td>0.261629</td>\n",
       "      <td>0.270172</td>\n",
       "      <td>0.274471</td>\n",
       "      <td>-1.356547</td>\n",
       "      <td>Thule</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>-0.653730</td>\n",
       "      <td>-0.619093</td>\n",
       "      <td>-0.590577</td>\n",
       "      <td>-0.566641</td>\n",
       "      <td>-0.545942</td>\n",
       "      <td>-0.527398</td>\n",
       "      <td>-0.508110</td>\n",
       "      <td>-0.489623</td>\n",
       "      <td>-0.471062</td>\n",
       "      <td>-0.451295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289864</td>\n",
       "      <td>0.302467</td>\n",
       "      <td>0.315095</td>\n",
       "      <td>0.328036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.259637</td>\n",
       "      <td>Tornio</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2940 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.45     0.475       0.5     0.525      0.55     0.575       0.6  \\\n",
       "0          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2935       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2936       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2937       NaN       NaN       NaN -0.760226 -0.727909 -0.699084 -0.676065   \n",
       "2938 -0.424504 -0.432160 -0.434251 -0.432117 -0.432182 -0.426685 -0.424460   \n",
       "2939 -0.653730 -0.619093 -0.590577 -0.566641 -0.545942 -0.527398 -0.508110   \n",
       "\n",
       "         0.625      0.65     0.675  ...      2.25       2.3      2.35  \\\n",
       "0          NaN       NaN       NaN  ...  0.178308  0.179383  0.177941   \n",
       "1          NaN       NaN       NaN  ...  0.231438  0.232110  0.218402   \n",
       "2          NaN       NaN       NaN  ...  0.220376  0.226100  0.222016   \n",
       "3          NaN       NaN       NaN  ...  0.154970  0.151323  0.144752   \n",
       "4          NaN       NaN       NaN  ...  0.210976  0.201246       NaN   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2935       NaN       NaN       NaN  ...  0.320975  0.337816  0.349726   \n",
       "2936       NaN       NaN       NaN  ...  0.279271  0.297687  0.316016   \n",
       "2937 -0.651195 -0.622867 -0.595565  ...  0.217393  0.225075  0.236595   \n",
       "2938 -0.413358 -0.403189 -0.394357  ...  0.240991  0.251120  0.261629   \n",
       "2939 -0.489623 -0.471062 -0.451295  ...  0.289864  0.302467  0.315095   \n",
       "\n",
       "           2.4      2.45        pV        name  counts  class_bdm  \\\n",
       "0     0.173358  0.168305       NaN     1988 TA       1          A   \n",
       "1          NaN       NaN -0.391474    1990 WO3       1          A   \n",
       "2     0.222024  0.240588       NaN    1993 FO6       1          A   \n",
       "3     0.138640       NaN -0.655608  1998 FE118       1         Sa   \n",
       "4          NaN       NaN -0.422508    1998 YG8       1          A   \n",
       "...        ...       ...       ...         ...     ...        ...   \n",
       "2935  0.362664       NaN -0.657577     Skorina       3          D   \n",
       "2936  0.332846  0.346252 -0.958607     Skorina       1          D   \n",
       "2937  0.251763  0.275564 -1.173925     Thestor       1          D   \n",
       "2938  0.270172  0.274471 -1.356547       Thule       1          D   \n",
       "2939  0.328036       NaN -1.259637      Tornio       1          D   \n",
       "\n",
       "      class_asteroid_sf  \n",
       "0                     A  \n",
       "1                     A  \n",
       "2                     A  \n",
       "3                     A  \n",
       "4                     A  \n",
       "...                 ...  \n",
       "2935                  Z  \n",
       "2936                  Z  \n",
       "2937                  Z  \n",
       "2938                  Z  \n",
       "2939                  Z  \n",
       "\n",
       "[2940 rows x 58 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import random\n",
    "csv_file_path = '02-BaseNew.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df_spectra = df.iloc[:, 0:53]\n",
    "albedo_column = df.iloc[:, 53]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52334cb8-5c61-4f39-908e-3e4f4c6d8846",
   "metadata": {},
   "source": [
    "- Tranform the spectra in reverse of their presented natural logarithm-transformed state.\n",
    "- Similarly, transform the albedo values in reverse of the transformed log base 10.\n",
    "- (Both transformations were required by Mahlke, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fe2c32a1-3b95-4ad7-8acb-8c61404df06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.475</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.525</th>\n",
       "      <th>0.55</th>\n",
       "      <th>0.575</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.625</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.675</th>\n",
       "      <th>...</th>\n",
       "      <th>2.25</th>\n",
       "      <th>2.3</th>\n",
       "      <th>2.35</th>\n",
       "      <th>2.4</th>\n",
       "      <th>2.45</th>\n",
       "      <th>pV</th>\n",
       "      <th>name</th>\n",
       "      <th>counts</th>\n",
       "      <th>class_bdm</th>\n",
       "      <th>class_asteroid_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195193</td>\n",
       "      <td>1.196479</td>\n",
       "      <td>1.194755</td>\n",
       "      <td>1.189291</td>\n",
       "      <td>1.183298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1988 TA</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.260412</td>\n",
       "      <td>1.261258</td>\n",
       "      <td>1.244087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1990 WO3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246545</td>\n",
       "      <td>1.253701</td>\n",
       "      <td>1.248591</td>\n",
       "      <td>1.248602</td>\n",
       "      <td>1.271996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993 FO6</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167623</td>\n",
       "      <td>1.163372</td>\n",
       "      <td>1.155753</td>\n",
       "      <td>1.148710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.221</td>\n",
       "      <td>1998 FE118</td>\n",
       "      <td>1</td>\n",
       "      <td>Sa</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234882</td>\n",
       "      <td>1.222925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378</td>\n",
       "      <td>1998 YG8</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.378472</td>\n",
       "      <td>1.401883</td>\n",
       "      <td>1.418678</td>\n",
       "      <td>1.437153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.220</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322165</td>\n",
       "      <td>1.346740</td>\n",
       "      <td>1.371653</td>\n",
       "      <td>1.394933</td>\n",
       "      <td>1.413759</td>\n",
       "      <td>0.110</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.467561</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>0.497040</td>\n",
       "      <td>0.508614</td>\n",
       "      <td>0.521423</td>\n",
       "      <td>0.536404</td>\n",
       "      <td>0.551251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.242833</td>\n",
       "      <td>1.252417</td>\n",
       "      <td>1.266928</td>\n",
       "      <td>1.286292</td>\n",
       "      <td>1.317274</td>\n",
       "      <td>0.067</td>\n",
       "      <td>Thestor</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>0.654094</td>\n",
       "      <td>0.649106</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.649133</td>\n",
       "      <td>0.649091</td>\n",
       "      <td>0.652669</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>0.661425</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.674114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.272509</td>\n",
       "      <td>1.285464</td>\n",
       "      <td>1.299044</td>\n",
       "      <td>1.310190</td>\n",
       "      <td>1.315834</td>\n",
       "      <td>0.044</td>\n",
       "      <td>Thule</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>0.520102</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.554007</td>\n",
       "      <td>0.567428</td>\n",
       "      <td>0.579296</td>\n",
       "      <td>0.590139</td>\n",
       "      <td>0.601631</td>\n",
       "      <td>0.612858</td>\n",
       "      <td>0.624339</td>\n",
       "      <td>0.636803</td>\n",
       "      <td>...</td>\n",
       "      <td>1.336246</td>\n",
       "      <td>1.353193</td>\n",
       "      <td>1.370389</td>\n",
       "      <td>1.388239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055</td>\n",
       "      <td>Tornio</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2940 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.45     0.475       0.5     0.525      0.55     0.575       0.6  \\\n",
       "0          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2935       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2936       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2937       NaN       NaN       NaN  0.467561  0.482917  0.497040  0.508614   \n",
       "2938  0.654094  0.649106  0.647750  0.649133  0.649091  0.652669  0.654123   \n",
       "2939  0.520102  0.538432  0.554007  0.567428  0.579296  0.590139  0.601631   \n",
       "\n",
       "         0.625      0.65     0.675  ...      2.25       2.3      2.35  \\\n",
       "0          NaN       NaN       NaN  ...  1.195193  1.196479  1.194755   \n",
       "1          NaN       NaN       NaN  ...  1.260412  1.261258  1.244087   \n",
       "2          NaN       NaN       NaN  ...  1.246545  1.253701  1.248591   \n",
       "3          NaN       NaN       NaN  ...  1.167623  1.163372  1.155753   \n",
       "4          NaN       NaN       NaN  ...  1.234882  1.222925       NaN   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2935       NaN       NaN       NaN  ...  1.378472  1.401883  1.418678   \n",
       "2936       NaN       NaN       NaN  ...  1.322165  1.346740  1.371653   \n",
       "2937  0.521423  0.536404  0.551251  ...  1.242833  1.252417  1.266928   \n",
       "2938  0.661425  0.668186  0.674114  ...  1.272509  1.285464  1.299044   \n",
       "2939  0.612858  0.624339  0.636803  ...  1.336246  1.353193  1.370389   \n",
       "\n",
       "           2.4      2.45     pV        name  counts  class_bdm  \\\n",
       "0     1.189291  1.183298    NaN     1988 TA       1          A   \n",
       "1          NaN       NaN  0.406    1990 WO3       1          A   \n",
       "2     1.248602  1.271996    NaN    1993 FO6       1          A   \n",
       "3     1.148710       NaN  0.221  1998 FE118       1         Sa   \n",
       "4          NaN       NaN  0.378    1998 YG8       1          A   \n",
       "...        ...       ...    ...         ...     ...        ...   \n",
       "2935  1.437153       NaN  0.220     Skorina       3          D   \n",
       "2936  1.394933  1.413759  0.110     Skorina       1          D   \n",
       "2937  1.286292  1.317274  0.067     Thestor       1          D   \n",
       "2938  1.310190  1.315834  0.044       Thule       1          D   \n",
       "2939  1.388239       NaN  0.055      Tornio       1          D   \n",
       "\n",
       "      class_asteroid_sf  \n",
       "0                     A  \n",
       "1                     A  \n",
       "2                     A  \n",
       "3                     A  \n",
       "4                     A  \n",
       "...                 ...  \n",
       "2935                  Z  \n",
       "2936                  Z  \n",
       "2937                  Z  \n",
       "2938                  Z  \n",
       "2939                  Z  \n",
       "\n",
       "[2940 rows x 58 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse natural logarithm transformation for spectra\n",
    "df.iloc[:, 0:53] = np.exp(df_spectra)\n",
    "# Reverse log base 10 transformation for albedo\n",
    "df.iloc[:, 53] = 10**albedo_column\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404a3c5-4b6d-4eaf-9a26-a829e8b39d8e",
   "metadata": {},
   "source": [
    "## Final method - Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef17e55-8676-4239-866c-cf1947b61c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaoj\\AppData\\Local\\Temp\\ipykernel_24952\\1160190029.py:415: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  pdf.savefig(fig, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF of all imputed spectra exported to 'all_imputedNEW.pdf'.\n",
      "CSV of final imputed spectra exported to '03-Base-imputedNEW.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import random\n",
    "\n",
    "# Set global font to DejaVu Serif and increase font sizes\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# ---------------------------\n",
    "# Load Data\n",
    "# ---------------------------\n",
    "\n",
    "# Assume spectral data are in the first 53 columns.\n",
    "df_spectra = df.iloc[:, 0:53].copy()\n",
    "total_cols = df_spectra.shape[1]  # typically 53\n",
    "wavelengths = np.array([float(col) for col in df_spectra.columns])\n",
    "\n",
    "# ---------------------------\n",
    "# Load class labels from column \"class_asteroid_sf\".\n",
    "# ---------------------------\n",
    "if 'class_asteroid_sf' in df.columns:\n",
    "    classes = df['class_asteroid_sf']\n",
    "else:\n",
    "    raise ValueError(\"No 'class_asteroid_sf' column found in the CSV data.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Imputation Parameters\n",
    "# ---------------------------\n",
    "overlap_points = 21   # number of points used for overlapping region\n",
    "slope_weight = 1.0    # slope weight = 1 for all imputation\n",
    "\n",
    "# ---------------------------\n",
    "# Function to compute error metric for a candidate.\n",
    "# Uses:\n",
    "#   - MSE computed with average shift over the overlapping region.\n",
    "#   - Slope difference computed after aligning using the \"i point\":\n",
    "#       * For left-incomplete: use the first overlapping point.\n",
    "#       * For right-incomplete: use the last overlapping point.\n",
    "# ---------------------------\n",
    "def compute_similarity_aligned(target, candidate, indices, slope_weight, side=\"left\"):\n",
    "    shift_avg = np.mean(target[indices] - candidate[indices])\n",
    "    candidate_aligned_avg = candidate + shift_avg\n",
    "    mse = np.mean((target[indices] - candidate_aligned_avg[indices])**2)\n",
    "    \n",
    "    if side == \"left\":\n",
    "        shift_point = target[indices[0]] - candidate[indices[0]]\n",
    "    else:\n",
    "        shift_point = target[indices[-1]] - candidate[indices[-1]]\n",
    "    candidate_aligned = candidate + shift_point\n",
    "    if len(indices) > 1:\n",
    "        target_slopes = np.diff(target[indices])\n",
    "        candidate_slopes = np.diff(candidate_aligned[indices])\n",
    "        slope_diff = np.mean(np.abs(target_slopes - candidate_slopes))\n",
    "    else:\n",
    "        slope_diff = 0.0\n",
    "    total_error = mse + slope_weight * slope_diff\n",
    "    return total_error\n",
    "\n",
    "# ---------------------------\n",
    "# Process all spectra and perform imputation for incomplete ones.\n",
    "# We'll store the final imputed spectra in final_imputed_list.\n",
    "# ---------------------------\n",
    "final_imputed_list = [None] * len(df_spectra)\n",
    "\n",
    "# Export a single PDF with one page per imputed (incomplete) spectrum.\n",
    "pdf = PdfPages(\"all_imputed.pdf\")\n",
    "\n",
    "for i in range(len(df_spectra)):\n",
    "    sample_orig = df_spectra.iloc[i].values.astype(float)\n",
    "    # Use original for candidate selection.\n",
    "    sample = sample_orig.copy()\n",
    "    # If spectrum is complete, leave it unchanged. Here the iteration starts.\n",
    "    if not (np.isnan(sample[0]) or np.isnan(sample[-1])):\n",
    "        final_imputed_list[i] = sample\n",
    "        continue\n",
    "\n",
    "    final_imputed = sample_orig.copy()  # working copy for final imputation\n",
    "    processed_left = False\n",
    "    processed_right = False\n",
    "\n",
    "    # ---- Left-side imputation ----\n",
    "    if np.isnan(sample[0]):\n",
    "        processed_left = True\n",
    "        first_obs = np.where(~np.isnan(sample))[0][0]\n",
    "        last_ob = np.where(~np.isnan(sample))[0][-1]\n",
    "        left_missing_indices = np.arange(0, first_obs)\n",
    "        left_overlap_indices = np.arange(first_obs, last_ob + 1)\n",
    "         \n",
    "        # Find candidate spectra with complete data in left missing & overlapping regions,\n",
    "        # and from the same class as the target.\n",
    "        candidates_left = []\n",
    "        for j in range(len(df_spectra)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            if classes[j] != classes[i]:\n",
    "                continue\n",
    "            cand = df_spectra.iloc[j].values.astype(float)\n",
    "            if np.all(~np.isnan(cand[left_overlap_indices])) and np.all(~np.isnan(cand[left_missing_indices])):\n",
    "                candidates_left.append((j, cand))\n",
    "        \n",
    "        # --- For targets of class \"R\", use all available candidates even if fewer than 10.\n",
    "        if classes[i] == \"R\":\n",
    "            if len(candidates_left) >= 1:\n",
    "                candidates_with_error_left = []\n",
    "                for cand_index, cand in candidates_left:\n",
    "                    err = compute_similarity_aligned(sample_orig, cand, left_overlap_indices, slope_weight, side=\"left\")\n",
    "                    candidates_with_error_left.append((cand_index, cand, err))\n",
    "                candidates_with_error_left.sort(key=lambda x: x[2])\n",
    "                best_candidates_left = candidates_with_error_left  # use all available candidates\n",
    "                imputed_left = []\n",
    "                errors_left = []\n",
    "                for cand_index, cand, err in best_candidates_left:\n",
    "                    errors_left.append(err)\n",
    "                    shift_point = sample_orig[left_overlap_indices[0]] - cand[left_overlap_indices[0]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    imputed_left.append(cand_aligned[left_missing_indices])\n",
    "                imputed_left = np.array(imputed_left)\n",
    "                errors_left = np.array(errors_left)\n",
    "                weights_left = 1.0 / (errors_left + 1e-6)\n",
    "                weighted_imputed_left = np.average(imputed_left, axis=0, weights=weights_left)\n",
    "                \n",
    "                # Smoothing: blend the first four missing points with extrapolated trend.\n",
    "                if len(left_overlap_indices) >= 2:\n",
    "                    x0 = wavelengths[left_overlap_indices[0]]\n",
    "                    x1 = wavelengths[left_overlap_indices[1]]\n",
    "                    y0 = sample_orig[left_overlap_indices[0]]\n",
    "                    y1 = sample_orig[left_overlap_indices[1]]\n",
    "                    slope_left = (y1 - y0) / (x1 - x0)\n",
    "                else:\n",
    "                    slope_left = 0\n",
    "                smoothed_left = weighted_imputed_left.copy()\n",
    "                for idx_missing, m in enumerate(left_missing_indices):\n",
    "                    d = first_obs - m  # distance from the boundary\n",
    "                    if d == 1:\n",
    "                        w = 0.5\n",
    "                    elif d == 2:\n",
    "                        w = 0.35\n",
    "                    elif d == 3:\n",
    "                        w = 0.25\n",
    "                    elif d == 4:\n",
    "                        w = 0.15\n",
    "                    elif d == 5:\n",
    "                        w = 0.1\n",
    "                    elif d == 6:\n",
    "                        w = 0.05\n",
    "                    elif d == 7:\n",
    "                        w = 0.0\n",
    "                    else:\n",
    "                        w = 0.0\n",
    "                    if w > 0:\n",
    "                        extrapolated_val = sample_orig[left_overlap_indices[0]] - slope_left * (x0 - wavelengths[m])\n",
    "                        smoothed_left[idx_missing] = w * extrapolated_val + (1 - w) * weighted_imputed_left[idx_missing]\n",
    "                weighted_imputed_left = smoothed_left\n",
    "                \n",
    "                final_imputed[left_missing_indices] = weighted_imputed_left\n",
    "                \n",
    "                # Plot left-side imputation.\n",
    "                fig, ax = plt.subplots(figsize=(10,6))\n",
    "                for cand_index, cand, err in best_candidates_left:\n",
    "                    shift_point = sample_orig[left_overlap_indices[0]] - cand[left_overlap_indices[0]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    ax.plot(wavelengths, cand_aligned, color='lightgray', linewidth=1, zorder=1)\n",
    "                ax.plot(wavelengths, sample_orig, 'ko-', label=\"Target (Observed)\", zorder=3)\n",
    "                ax.plot(wavelengths, final_imputed, 'b--', label=\"Final Imputed Spectrum\", zorder=4)\n",
    "                ax.scatter(wavelengths[left_missing_indices], final_imputed[left_missing_indices], color='red', \n",
    "                           label=\"Imputed Points\", zorder=5)\n",
    "                ax.axvspan(wavelengths[left_missing_indices[0]], wavelengths[left_missing_indices[-1]], \n",
    "                           color='red', alpha=0.2, label=\"Missing Region\", zorder=2)\n",
    "                ax.set_xlabel(\"Wavelength (µm)\")\n",
    "                ax.set_ylabel(\"Reflectance\")\n",
    "                ax.legend()\n",
    "                ax.text(0.5, -0.2, f\"Target {i}: Left Imputation\", transform=ax.transAxes, \n",
    "                        ha='center', va='center', fontsize=20)\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        else:\n",
    "            if len(candidates_left) < 10:\n",
    "                pass  # Skip left-imputation if not enough candidates.\n",
    "            else:\n",
    "                candidates_with_error_left = []\n",
    "                for cand_index, cand in candidates_left:\n",
    "                    err = compute_similarity_aligned(sample_orig, cand, left_overlap_indices, slope_weight, side=\"left\")\n",
    "                    candidates_with_error_left.append((cand_index, cand, err))\n",
    "                candidates_with_error_left.sort(key=lambda x: x[2])\n",
    "                best_candidates_left = candidates_with_error_left[:10]\n",
    "                \n",
    "                imputed_left = []\n",
    "                errors_left = []\n",
    "                for cand_index, cand, err in best_candidates_left:\n",
    "                    errors_left.append(err)\n",
    "                    shift_point = sample_orig[left_overlap_indices[0]] - cand[left_overlap_indices[0]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    imputed_left.append(cand_aligned[left_missing_indices])\n",
    "                imputed_left = np.array(imputed_left)\n",
    "                errors_left = np.array(errors_left)\n",
    "                weights_left = 1.0 / (errors_left + 1e-6)\n",
    "                weighted_imputed_left = np.average(imputed_left, axis=0, weights=weights_left)\n",
    "                \n",
    "                if len(left_overlap_indices) >= 2:\n",
    "                    x0 = wavelengths[left_overlap_indices[0]]\n",
    "                    x1 = wavelengths[left_overlap_indices[1]]\n",
    "                    y0 = sample_orig[left_overlap_indices[0]]\n",
    "                    y1 = sample_orig[left_overlap_indices[1]]\n",
    "                    slope_left = (y1 - y0) / (x1 - x0)\n",
    "                else:\n",
    "                    slope_left = 0\n",
    "                smoothed_left = weighted_imputed_left.copy()\n",
    "                for idx_missing, m in enumerate(left_missing_indices):\n",
    "                    d = first_obs - m\n",
    "                    if d == 1:\n",
    "                        w = 0.5\n",
    "                    elif d == 2:\n",
    "                        w = 0.35\n",
    "                    elif d == 3:\n",
    "                        w = 0.25\n",
    "                    elif d == 4:\n",
    "                        w = 0.15\n",
    "                    elif d == 5:\n",
    "                        w = 0.1\n",
    "                    elif d == 6:\n",
    "                        w = 0.05\n",
    "                    elif d == 7:\n",
    "                        w = 0.0\n",
    "                    else:\n",
    "                        w = 0.0\n",
    "                    if w > 0:\n",
    "                        extrapolated_val = sample_orig[left_overlap_indices[0]] - slope_left * (x0 - wavelengths[m])\n",
    "                        smoothed_left[idx_missing] = w * extrapolated_val + (1 - w) * weighted_imputed_left[idx_missing]\n",
    "                weighted_imputed_left = smoothed_left\n",
    "                \n",
    "                final_imputed[left_missing_indices] = weighted_imputed_left\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10,6))\n",
    "                for cand_index, cand, err in best_candidates_left:\n",
    "                    shift_point = sample_orig[left_overlap_indices[0]] - cand[left_overlap_indices[0]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    ax.plot(wavelengths, cand_aligned, color='lightgray', linewidth=1, zorder=1)\n",
    "                ax.plot(wavelengths, sample_orig, 'ko-', label=\"Target (Observed)\", zorder=3)\n",
    "                ax.plot(wavelengths, final_imputed, 'b--', label=\"Final Imputed Spectrum\", zorder=4)\n",
    "                ax.scatter(wavelengths[left_missing_indices], final_imputed[left_missing_indices], color='red', \n",
    "                           label=\"Imputed Points\", zorder=5)\n",
    "                ax.axvspan(wavelengths[left_missing_indices[0]], wavelengths[left_missing_indices[-1]], \n",
    "                           color='red', alpha=0.2, label=\"Missing Region\", zorder=2)\n",
    "                ax.set_xlabel(\"Wavelength (µm)\")\n",
    "                ax.set_ylabel(\"Reflectance\")\n",
    "                ax.legend()\n",
    "                ax.text(0.5, -0.2, f\"Target {i}: Left Imputation\", transform=ax.transAxes, \n",
    "                        ha='center', va='center', fontsize=20)\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "    # ---- Right-side imputation ----\n",
    "    if np.isnan(sample[-1]):\n",
    "        processed_right = True\n",
    "        last_obs = np.where(~np.isnan(sample))[0][-1]\n",
    "        first_ob = np.where(~np.isnan(sample))[0][0]\n",
    "        right_missing_indices = np.arange(last_obs + 1, total_cols)\n",
    "        right_overlap_indices = np.arange(first_ob, last_obs + 1)\n",
    "        \n",
    "        candidates_right = []\n",
    "        for j in range(len(df_spectra)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            if classes[j] != classes[i]:\n",
    "                continue\n",
    "            cand = df_spectra.iloc[j].values.astype(float)\n",
    "            if np.all(~np.isnan(cand[right_overlap_indices])) and np.all(~np.isnan(cand[right_missing_indices])):\n",
    "                candidates_right.append((j, cand))\n",
    "                \n",
    "        if classes[i] == \"R\":\n",
    "            if len(candidates_right) >= 1:\n",
    "                candidates_with_error_right = []\n",
    "                for cand_index, cand in candidates_right:\n",
    "                    err = compute_similarity_aligned(sample_orig, cand, right_overlap_indices, slope_weight, side=\"right\")\n",
    "                    candidates_with_error_right.append((cand_index, cand, err))\n",
    "                candidates_with_error_right.sort(key=lambda x: x[2])\n",
    "                best_candidates_right = candidates_with_error_right  # use all available candidates\n",
    "                imputed_right = []\n",
    "                errors_right = []\n",
    "                for cand_index, cand, err in best_candidates_right:\n",
    "                    errors_right.append(err)\n",
    "                    shift_point = sample_orig[right_overlap_indices[-1]] - cand[right_overlap_indices[-1]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    imputed_right.append(cand_aligned[right_missing_indices])\n",
    "                imputed_right = np.array(imputed_right)\n",
    "                errors_right = np.array(errors_right)\n",
    "                weights_right = 1.0 / (errors_right + 1e-6)\n",
    "                weighted_imputed_right = np.average(imputed_right, axis=0, weights=weights_right)\n",
    "                \n",
    "                if len(right_overlap_indices) >= 2:\n",
    "                    x0 = wavelengths[right_overlap_indices[-2]]\n",
    "                    x1 = wavelengths[right_overlap_indices[-1]]\n",
    "                    y0 = sample_orig[right_overlap_indices[-2]]\n",
    "                    y1 = sample_orig[right_overlap_indices[-1]]\n",
    "                    slope_right = (y1 - y0) / (x1 - x0)\n",
    "                else:\n",
    "                    slope_right = 0\n",
    "                smoothed_right = weighted_imputed_right.copy()\n",
    "                for idx_missing, m in enumerate(right_missing_indices):\n",
    "                    d = m - last_obs  # distance from boundary\n",
    "                    if d == 1:\n",
    "                        w = 0.5\n",
    "                    elif d == 2:\n",
    "                        w = 0.35\n",
    "                    elif d == 3:\n",
    "                        w = 0.25\n",
    "                    elif d == 4:\n",
    "                        w = 0.15\n",
    "                    elif d == 5:\n",
    "                        w = 0.1\n",
    "                    elif d == 6:\n",
    "                        w = 0.05\n",
    "                    elif d == 7:\n",
    "                        w = 0.0\n",
    "                    else:\n",
    "                        w = 0.0\n",
    "                    if w > 0:\n",
    "                        extrapolated_val = sample_orig[right_overlap_indices[-1]] + slope_right * (wavelengths[m] - wavelengths[right_overlap_indices[-1]])\n",
    "                        smoothed_right[idx_missing] = w * extrapolated_val + (1 - w) * weighted_imputed_right[idx_missing]\n",
    "                weighted_imputed_right = smoothed_right\n",
    "                final_imputed[right_missing_indices] = weighted_imputed_right\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10,6))\n",
    "                for cand_index, cand, err in best_candidates_right:\n",
    "                    shift_point = sample_orig[right_overlap_indices[-1]] - cand[right_overlap_indices[-1]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    ax.plot(wavelengths, cand_aligned, color='lightgray', linewidth=1, zorder=1)\n",
    "                ax.plot(wavelengths, sample_orig, 'ko-', label=\"Target (Observed)\", zorder=3)\n",
    "                ax.plot(wavelengths, final_imputed, 'b--', label=\"Final Imputed Spectrum\", zorder=4)\n",
    "                ax.scatter(wavelengths[right_missing_indices], final_imputed[right_missing_indices], color='red', \n",
    "                           label=\"Imputed Points\", zorder=5)\n",
    "                ax.axvspan(wavelengths[right_missing_indices[0]], wavelengths[right_missing_indices[-1]], \n",
    "                           color='red', alpha=0.2, label=\"Missing Region\", zorder=2)\n",
    "                ax.set_xlabel(\"Wavelength (µm)\")\n",
    "                ax.set_ylabel(\"Reflectance\")\n",
    "                ax.legend()\n",
    "                ax.text(0.5, -0.2, f\"Target {i}: Right Imputation\", transform=ax.transAxes, \n",
    "                        ha='center', va='center', fontsize=20)\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        else:\n",
    "            if len(candidates_right) < 10:\n",
    "                pass\n",
    "            else:\n",
    "                candidates_with_error_right = []\n",
    "                for cand_index, cand in candidates_right:\n",
    "                    err = compute_similarity_aligned(sample_orig, cand, right_overlap_indices, slope_weight, side=\"right\")\n",
    "                    candidates_with_error_right.append((cand_index, cand, err))\n",
    "                candidates_with_error_right.sort(key=lambda x: x[2])\n",
    "                best_candidates_right = candidates_with_error_right[:10]\n",
    "                \n",
    "                imputed_right = []\n",
    "                errors_right = []\n",
    "                for cand_index, cand, err in best_candidates_right:\n",
    "                    errors_right.append(err)\n",
    "                    shift_point = sample_orig[right_overlap_indices[-1]] - cand[right_overlap_indices[-1]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    imputed_right.append(cand_aligned[right_missing_indices])\n",
    "                imputed_right = np.array(imputed_right)\n",
    "                errors_right = np.array(errors_right)\n",
    "                weights_right = 1.0 / (errors_right + 1e-6)\n",
    "                weighted_imputed_right = np.average(imputed_right, axis=0, weights=weights_right)\n",
    "                \n",
    "                if len(right_overlap_indices) >= 2:\n",
    "                    x0 = wavelengths[right_overlap_indices[-2]]\n",
    "                    x1 = wavelengths[right_overlap_indices[-1]]\n",
    "                    y0 = sample_orig[right_overlap_indices[-2]]\n",
    "                    y1 = sample_orig[right_overlap_indices[-1]]\n",
    "                    slope_right = (y1 - y0) / (x1 - x0)\n",
    "                else:\n",
    "                    slope_right = 0\n",
    "                smoothed_right = weighted_imputed_right.copy()\n",
    "                for idx_missing, m in enumerate(right_missing_indices):\n",
    "                    d = m - last_obs\n",
    "                    if d == 1:\n",
    "                        w = 0.5\n",
    "                    elif d == 2:\n",
    "                        w = 0.35\n",
    "                    elif d == 3:\n",
    "                        w = 0.25\n",
    "                    elif d == 4:\n",
    "                        w = 0.15\n",
    "                    elif d == 5:\n",
    "                        w = 0.1\n",
    "                    elif d == 6:\n",
    "                        w = 0.05\n",
    "                    elif d == 7:\n",
    "                        w = 0.0\n",
    "                    else:\n",
    "                        w = 0.0\n",
    "                    if w > 0:\n",
    "                        extrapolated_val = sample_orig[right_overlap_indices[-1]] + slope_right * (wavelengths[m] - wavelengths[right_overlap_indices[-1]])\n",
    "                        smoothed_right[idx_missing] = w * extrapolated_val + (1 - w) * weighted_imputed_right[idx_missing]\n",
    "                weighted_imputed_right = smoothed_right\n",
    "                final_imputed[right_missing_indices] = weighted_imputed_right\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10,6))\n",
    "                for cand_index, cand, err in best_candidates_right:\n",
    "                    shift_point = sample_orig[right_overlap_indices[-1]] - cand[right_overlap_indices[-1]]\n",
    "                    cand_aligned = cand + shift_point\n",
    "                    ax.plot(wavelengths, cand_aligned, color='lightgray', linewidth=1, zorder=1)\n",
    "                ax.plot(wavelengths, sample_orig, 'ko-', label=\"Target (Observed)\", zorder=3)\n",
    "                ax.plot(wavelengths, final_imputed, 'b--', label=\"Final Imputed Spectrum\", zorder=4)\n",
    "                ax.scatter(wavelengths[right_missing_indices], final_imputed[right_missing_indices], color='red', \n",
    "                           label=\"Imputed Points\", zorder=5)\n",
    "                ax.axvspan(wavelengths[right_missing_indices[0]], wavelengths[right_missing_indices[-1]], \n",
    "                           color='red', alpha=0.2, label=\"Missing Region\", zorder=2)\n",
    "                ax.set_xlabel(\"Wavelength (µm)\")\n",
    "                ax.set_ylabel(\"Reflectance\")\n",
    "                ax.legend()\n",
    "                ax.text(0.5, -0.2, f\"Target {i}: Right Imputation\", transform=ax.transAxes, \n",
    "                        ha='center', va='center', fontsize=20)\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "    \n",
    "    final_imputed_list[i] = final_imputed\n",
    "\n",
    "pdf.close()\n",
    "\n",
    "# ---------------------------\n",
    "# Export final imputed spectra to CSV\n",
    "# ---------------------------\n",
    "df_imputed = pd.DataFrame(final_imputed_list, columns=df_spectra.columns, index=df_spectra.index)\n",
    "df_imputed.to_csv(\"03-Base-imputedNEW.csv\", index=False)\n",
    "\n",
    "print(\"PDF of all imputed spectra exported to 'all_imputedNEW.pdf'.\")\n",
    "print(\"CSV of final imputed spectra exported to '03-Base-imputedNEW.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f058db-2c6a-40eb-9fe5-21ee7f540dd1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your original DataFrame 'df' is already loaded\n",
    "# No dummy data generation here. We assume 'df' exists.\n",
    "\n",
    "# ---------------------------\n",
    "# Load Data\n",
    "# ---------------------------\n",
    "\n",
    "# Assume spectral data are in the first 53 columns.\n",
    "df_spectra = df.iloc[:, 0:53].copy()\n",
    "wavelengths = np.array([float(col) for col in df_spectra.columns])\n",
    "\n",
    "# Ensure the wavelength range is within 0.45 to 2.45\n",
    "valid_wavelength_indices = np.where((wavelengths >= 0.45) & (wavelengths <= 2.45))[0]\n",
    "# Check if any valid indices were found\n",
    "if len(valid_wavelength_indices) == 0:\n",
    "    raise ValueError(\"No wavelengths found in the 0.45 to 2.45 range in the first 53 columns.\")\n",
    "wavelengths_subset = wavelengths[valid_wavelength_indices]\n",
    "# Select the subset columns from the original df_spectra\n",
    "df_spectra_subset = df_spectra.iloc[:, valid_wavelength_indices].copy()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load class labels\n",
    "# ---------------------------\n",
    "if 'class_asteroid_sf' in df.columns:\n",
    "    classes = df['class_asteroid_sf']\n",
    "else:\n",
    "    raise ValueError(\"No 'class_asteroid_sf' column found in the CSV data.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define the target indices\n",
    "# ---------------------------\n",
    "target_indices_to_plot = [1339]  # Removed 1341\n",
    "slope_weight = 1.0  # You might need to adjust this weight\n",
    "\n",
    "# ---------------------------\n",
    "# Function to compute similarity aligned (from your reference)\n",
    "# ---------------------------\n",
    "def compute_similarity_aligned(target, candidate, indices, slope_weight, side=\"left\"):\n",
    "    # Ensure inputs are numpy arrays and indices is not empty\n",
    "    target = np.asarray(target)\n",
    "    candidate = np.asarray(candidate)\n",
    "    indices = np.asarray(indices) # Relative indices within the overlap part\n",
    "\n",
    "    if indices.size == 0:\n",
    "        return 1e9 # High error for no overlap\n",
    "\n",
    "    # Handle case where only one point overlaps\n",
    "    if indices.size < 2:\n",
    "        # Calculate MSE based on the single point\n",
    "        shift_point_single = target[indices[0]] - candidate[indices[0]]\n",
    "        candidate_aligned_single = candidate + shift_point_single\n",
    "        # MSE on the single point (after alignment) - could also just be the diff squared\n",
    "        # Let's calculate MSE based on mean shift even for one point for consistency, though slope is 0\n",
    "        shift_avg_single = target[indices[0]] - candidate[indices[0]] # Mean shift is just the shift\n",
    "        candidate_aligned_avg_single = candidate + shift_avg_single\n",
    "        mse_single = (target[indices[0]] - candidate_aligned_avg_single[indices[0]])**2\n",
    "        return mse_single # Return only MSE as slope_diff is 0 or undefined\n",
    "\n",
    "    # --- MSE Calculation (using mean shift) ---\n",
    "    # Use only the values corresponding to the relative indices\n",
    "    target_overlap = target[indices]\n",
    "    candidate_overlap = candidate[indices]\n",
    "\n",
    "    shift_avg = np.mean(target_overlap - candidate_overlap)\n",
    "    candidate_aligned_avg_overlap = candidate_overlap + shift_avg\n",
    "    mse = np.mean((target_overlap - candidate_aligned_avg_overlap)**2)\n",
    "\n",
    "    # --- Slope Difference Calculation (using point alignment) ---\n",
    "    # Align based on the first or last point of the *overlap*\n",
    "    if side == \"left\":\n",
    "        # Use relative index 0\n",
    "        shift_point = target_overlap[0] - candidate_overlap[0]\n",
    "    else: # side == \"right\"\n",
    "        # Use relative index -1\n",
    "        shift_point = target_overlap[-1] - candidate_overlap[-1]\n",
    "\n",
    "    candidate_aligned_point_overlap = candidate_overlap + shift_point\n",
    "\n",
    "    # Calculate slopes on the overlapping region using the point-aligned candidate overlap\n",
    "    target_slopes = np.diff(target_overlap)\n",
    "    candidate_slopes = np.diff(candidate_aligned_point_overlap)\n",
    "    slope_diff = np.mean(np.abs(target_slopes - candidate_slopes))\n",
    "\n",
    "    total_error = mse + slope_weight * slope_diff\n",
    "    return total_error if np.isfinite(total_error) else 1e9\n",
    "\n",
    "# ---------------------------\n",
    "# Function to compute MSE for alignment (Not directly used in final imputation, but used by find_top_n_candidates_mse)\n",
    "# ---------------------------\n",
    "def compute_mse(target, candidate, indices):\n",
    "    if not indices.size:\n",
    "        return float('inf')\n",
    "    # Ensure target and candidate are properly indexed if indices are relative\n",
    "    return np.mean((target[indices] - candidate[indices])**2)\n",
    "\n",
    "# ---------------------------\n",
    "# Function to align candidate based on minimum MSE (for plot 3)\n",
    "# This function aligns the *part* of the candidate corresponding to 'indices'.\n",
    "# It should likely return the *shift* or align the whole candidate based on this part.\n",
    "# Let's modify it slightly to return the aligned *part* for consistency with its use.\n",
    "# ---------------------------\n",
    "def align_candidate_mse(target_part, candidate_part, indices_relative):\n",
    "    \"\"\"Aligns candidate_part to target_part over relative indices using MSE minimum shift.\"\"\"\n",
    "    if not indices_relative.size or target_part.size != candidate_part.size:\n",
    "        return candidate_part # Return original part if no indices or size mismatch\n",
    "\n",
    "    target_subset = target_part[indices_relative]\n",
    "    candidate_subset = candidate_part[indices_relative]\n",
    "\n",
    "    if target_subset.size == 0: # Check if subset selection resulted in empty array\n",
    "        return candidate_part\n",
    "\n",
    "    best_shift = 0\n",
    "    min_mse = float('inf')\n",
    "\n",
    "    # Simplified: Calculate optimal shift directly\n",
    "    valid = ~np.isnan(target_subset) & ~np.isnan(candidate_subset)\n",
    "    if np.sum(valid) > 0:\n",
    "         best_shift = np.mean(target_subset[valid]) - np.mean(candidate_subset[valid])\n",
    "    else:\n",
    "         best_shift = 0 # No valid points to compare\n",
    "\n",
    "    # Apply the shift to the original candidate_part passed to the function\n",
    "    return candidate_part + best_shift\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Function to highlight contiguous regions\n",
    "# ---------------------------\n",
    "def highlight_contiguous_regions(ax, wavelengths, indices, color, alpha, label):\n",
    "    # Ensure indices is a numpy array\n",
    "    indices = np.asarray(indices)\n",
    "    if indices.size == 0:\n",
    "        return\n",
    "\n",
    "    indices = np.sort(indices) # Ensure indices are sorted\n",
    "\n",
    "    # Find start and end points of contiguous blocks\n",
    "    diff = np.diff(indices)\n",
    "    starts = np.concatenate(([indices[0]], indices[1:][diff > 1]))\n",
    "    ends = np.concatenate((indices[:-1][diff > 1], [indices[-1]]))\n",
    "\n",
    "    for i, (start_idx, end_idx) in enumerate(zip(starts, ends)):\n",
    "        current_label = label if i == 0 else None # Label only the first block\n",
    "        # Safety check for index bounds\n",
    "        wl_start = wavelengths[start_idx] if start_idx < len(wavelengths) else wavelengths[-1]\n",
    "        wl_end = wavelengths[end_idx] if end_idx < len(wavelengths) else wavelengths[-1]\n",
    "\n",
    "        if wl_start <= wl_end:\n",
    "             ax.axvspan(wl_start, wl_end, color=color, alpha=alpha, label=current_label, zorder=0)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Find top N candidate spectra based on MSE (for plot 3)\n",
    "# Uses the globally defined 'wavelengths' and 'valid_wavelength_indices'\n",
    "# Takes df_spectra_subset as input\n",
    "# ---------------------------\n",
    "def find_top_n_candidates_mse(df_spectra_subset_local, classes_local, target_index_local, target_spectrum_observed_part, observed_indices_subset_local, nan_indices_subset_local, n=10):\n",
    "    candidates_with_mse = []\n",
    "    target_class_local = classes_local.iloc[target_index_local]\n",
    "    num_observed = len(observed_indices_subset_local)\n",
    "\n",
    "    if num_observed == 0:\n",
    "        print(f\"Warning (MSE): Target {target_index_local} has no observed points. Cannot find MSE candidates.\")\n",
    "        return []\n",
    "\n",
    "    # Define the indices that the candidate *must* have non-NaN values for\n",
    "    # These are the observed indices of the target, and potentially the missing ones if needed later\n",
    "    # For MSE alignment, we only strictly need the observed overlap non-NaN\n",
    "    # Let's also check the left missing part needed for *potential* later use or consistency\n",
    "    first_obs_idx = observed_indices_subset_local[0]\n",
    "    left_missing_indices_subset_needed = nan_indices_subset_local[nan_indices_subset_local < first_obs_idx]\n",
    "\n",
    "    for i in range(len(df_spectra_subset_local)):\n",
    "        # Use iloc index i for df_spectra_subset_local, but check original index 'target_index_local' against df's index\n",
    "        original_df_index = df_spectra_subset_local.index[i]\n",
    "        if original_df_index == target_index_local or classes_local.loc[original_df_index] != target_class_local:\n",
    "            continue\n",
    "\n",
    "        candidate_subset = df_spectra_subset_local.iloc[i].values.astype(float)\n",
    "\n",
    "        # Check if candidate has NaNs in the target's observed region or the target's left missing region\n",
    "        has_nan_in_observed = np.any(np.isnan(candidate_subset[observed_indices_subset_local]))\n",
    "        has_nan_in_left_missing = np.any(np.isnan(candidate_subset[left_missing_indices_subset_needed]))\n",
    "\n",
    "        if not has_nan_in_observed and not has_nan_in_left_missing:\n",
    "            candidate_observed_part = candidate_subset[observed_indices_subset_local]\n",
    "\n",
    "            # Align the candidate's observed part to the target's observed part\n",
    "            # align_candidate_mse now returns the *aligned candidate part*\n",
    "            aligned_candidate_part = align_candidate_mse(target_spectrum_observed_part, candidate_observed_part, np.arange(num_observed))\n",
    "\n",
    "            # Calculate MSE between target observed part and the *aligned* candidate part\n",
    "            mse = np.mean((target_spectrum_observed_part - aligned_candidate_part)**2)\n",
    "\n",
    "            if np.isfinite(mse):\n",
    "                # Store the original candidate subset spectrum and its alignment MSE\n",
    "                candidates_with_mse.append((candidate_subset, mse)) # Store tuple (spectrum, mse)\n",
    "\n",
    "    candidates_with_mse.sort(key=lambda item: item[1]) # Sort by MSE (second element)\n",
    "    # Return only the spectra of the top N candidates\n",
    "    top_candidates_spectra = [item[0] for item in candidates_with_mse[:n]]\n",
    "    return top_candidates_spectra\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Find top N candidate spectra based on similarity (MSE + Slope)\n",
    "# Uses the globally defined 'wavelengths' and 'valid_wavelength_indices'\n",
    "# Takes df_spectra_subset as input\n",
    "# **MODIFIED TO RETURN LIST OF (spectrum, error) TUPLES**\n",
    "# ---------------------------\n",
    "def find_top_n_candidates(df_spectra_subset_local, classes_local, target_index_local, target_spectrum_observed_part, observed_indices_subset_local, nan_indices_subset_local, slope_weight_local, n=10, side=\"left\"):\n",
    "    candidates_with_error = []\n",
    "    target_class_local = classes_local.iloc[target_index_local]\n",
    "    num_observed = len(observed_indices_subset_local)\n",
    "\n",
    "    # Need at least 2 observed points to calculate slope difference within compute_similarity_aligned\n",
    "    if num_observed < 2:\n",
    "        print(f\"Warning (Similarity): Target {target_index_local} has < 2 observed points. Cannot reliably calculate similarity including slope.\")\n",
    "        # Return empty list as the similarity metric relies on slope diff\n",
    "        return []\n",
    "\n",
    "    # Define the indices that the candidate *must* have non-NaN values for\n",
    "    first_obs_idx = observed_indices_subset_local[0]\n",
    "    left_missing_indices_subset_needed = nan_indices_subset_local[nan_indices_subset_local < first_obs_idx]\n",
    "\n",
    "    for i in range(len(df_spectra_subset_local)):\n",
    "        original_df_index = df_spectra_subset_local.index[i]\n",
    "        if original_df_index == target_index_local or classes_local.loc[original_df_index] != target_class_local:\n",
    "            continue\n",
    "\n",
    "        candidate_subset = df_spectra_subset_local.iloc[i].values.astype(float)\n",
    "\n",
    "        # Check if candidate has NaNs in the target's observed region or the target's left missing region\n",
    "        has_nan_in_observed = np.any(np.isnan(candidate_subset[observed_indices_subset_local]))\n",
    "        has_nan_in_left_missing = np.any(np.isnan(candidate_subset[left_missing_indices_subset_needed]))\n",
    "\n",
    "        if not has_nan_in_observed and not has_nan_in_left_missing:\n",
    "            candidate_observed_part = candidate_subset[observed_indices_subset_local]\n",
    "\n",
    "            # Compute similarity using the observed parts and relative indices\n",
    "            error = compute_similarity_aligned(target_spectrum_observed_part, candidate_observed_part,\n",
    "                                               np.arange(num_observed), # Relative indices for the overlap part\n",
    "                                               slope_weight_local, side=side)\n",
    "\n",
    "            if np.isfinite(error):\n",
    "                 candidates_with_error.append((candidate_subset, error)) # Store tuple (spectrum, error)\n",
    "\n",
    "\n",
    "    candidates_with_error.sort(key=lambda item: item[1]) # Sort by error (second element)\n",
    "\n",
    "    # *** MODIFICATION: Return the list of tuples (spectrum, error) ***\n",
    "    return candidates_with_error[:n]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Function to align a candidate (using point alignment for plotting)\n",
    "# Aligns the *entire* candidate spectrum based on one point in the overlap.\n",
    "# ---------------------------\n",
    "def align_candidate_point(target_subset, candidate_subset, anchor_indices_subset, side=\"left\"):\n",
    "    \"\"\"Aligns the full candidate_subset based on an anchor point comparison.\"\"\"\n",
    "    if not anchor_indices_subset.size:\n",
    "        return candidate_subset # No anchor point provided\n",
    "\n",
    "    if side == \"left\":\n",
    "        anchor_idx = anchor_indices_subset[0]\n",
    "    else: # side == \"right\"\n",
    "        anchor_idx = anchor_indices_subset[-1]\n",
    "\n",
    "    # Check if anchor point is valid in both target and candidate\n",
    "    if anchor_idx >= len(target_subset) or anchor_idx >= len(candidate_subset) or \\\n",
    "       np.isnan(target_subset[anchor_idx]) or np.isnan(candidate_subset[anchor_idx]):\n",
    "        # print(f\"Warning: Cannot align using anchor index {anchor_idx}. Values invalid or out of bounds.\")\n",
    "        return candidate_subset # Return unaligned if anchor is bad\n",
    "\n",
    "    shift = target_subset[anchor_idx] - candidate_subset[anchor_idx]\n",
    "    return candidate_subset + shift\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot the specified target spectra\n",
    "# ---------------------------\n",
    "print(f\"Starting analysis for target indices: {target_indices_to_plot}\")\n",
    "for target_index in target_indices_to_plot:\n",
    "    # Check if target index exists in the original DataFrame index\n",
    "    if target_index not in df.index:\n",
    "         print(f\"Index {target_index} is out of bounds or not found in the DataFrame index. Skipping.\")\n",
    "         continue\n",
    "\n",
    "    # Get data using the subset DataFrame for spectra, but original index for class\n",
    "    # Use .loc to safely access row by index label from df_spectra_subset\n",
    "    if target_index not in df_spectra_subset.index:\n",
    "         print(f\"Index {target_index} not found in df_spectra_subset (possibly filtered out by wavelength range). Skipping.\")\n",
    "         continue\n",
    "\n",
    "    sample_subset = df_spectra_subset.loc[target_index].values.astype(float)\n",
    "    target_class = classes.loc[target_index] # Use original index for classes Series\n",
    "\n",
    "    print(f\"\\nProcessing Target Index: {target_index} (Class: {target_class})\")\n",
    "\n",
    "    # Check if it belongs to class 'Q' and has NaNs within the subset\n",
    "    if target_class == 'Q' and np.any(np.isnan(sample_subset)):\n",
    "        # ---------------------------\n",
    "        # Identify missing and observed indices (relative to sample_subset/wavelengths_subset)\n",
    "        # ---------------------------\n",
    "        nan_indices_subset = np.where(np.isnan(sample_subset))[0]\n",
    "        observed_indices_subset = np.where(~np.isnan(sample_subset))[0]\n",
    "\n",
    "        if observed_indices_subset.size == 0:\n",
    "             print(f\"Skipping index {target_index}: No observed data points in the 0.45-2.45 µm range.\")\n",
    "             continue\n",
    "\n",
    "        print(f\"  Observed indices count: {len(observed_indices_subset)}, NaN indices count: {len(nan_indices_subset)}\")\n",
    "\n",
    "        # ---------------------------\n",
    "        # Find Top Candidates\n",
    "        # ---------------------------\n",
    "\n",
    "        # --- Find top 10 candidates based on SIMILARITY (for imputation & plot 2) ---\n",
    "        # Note: find_top_n_candidates now returns list of (spectrum, error) tuples\n",
    "        top_candidates_left_info = []\n",
    "        target_observed_part = sample_subset[observed_indices_subset] # Get the observed part of the target\n",
    "        # Check if enough observed points for similarity calculation\n",
    "        if observed_indices_subset.size >= 2:\n",
    "             top_candidates_left_info = find_top_n_candidates(\n",
    "                 df_spectra_subset, classes, target_index, # Pass necessary args\n",
    "                 target_observed_part, # Pass only the observed part for comparison\n",
    "                 observed_indices_subset, nan_indices_subset,\n",
    "                 slope_weight, n=10, side=\"left\"\n",
    "             )\n",
    "             print(f\"  Found {len(top_candidates_left_info)} candidates based on similarity.\")\n",
    "        else:\n",
    "             print(f\"  Skipping similarity candidates: Need >= 2 observed points (found {observed_indices_subset.size}).\")\n",
    "\n",
    "\n",
    "        # --- Find top 10 candidates based on MSE alignment (for plot 3) ---\n",
    "        # Note: find_top_n_candidates_mse returns only spectra list\n",
    "        top_candidates_mse_aligned_spectra = []\n",
    "        if observed_indices_subset.size >= 1: # Need at least 1 point for MSE alignment\n",
    "            top_candidates_mse_aligned_spectra = find_top_n_candidates_mse(\n",
    "                 df_spectra_subset, classes, target_index, # Pass necessary args\n",
    "                 target_observed_part, # Pass only the observed part\n",
    "                 observed_indices_subset, nan_indices_subset, n=10\n",
    "             )\n",
    "            print(f\"  Found {len(top_candidates_mse_aligned_spectra)} candidates based on MSE.\")\n",
    "        else:\n",
    "             print(f\"  Skipping MSE candidates: Need >= 1 observed point.\")\n",
    "\n",
    "\n",
    "        # ---------------------------\n",
    "        # Create the figure with four subplots in a 2x2 grid\n",
    "        # ---------------------------\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 10), sharey=True) # Adjusted figsize slightly\n",
    "        fig.suptitle(f\"Analysis for Target Spectrum Index {target_index} (Class {target_class})\", fontsize=24, y=1.02)\n",
    "        common_xlim = (wavelengths_subset[0] - 0.05, wavelengths_subset[-1] + 0.05)\n",
    "\n",
    "\n",
    "        # --- Plot on the first subplot (axes[0, 0]) ---\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.plot(wavelengths_subset, sample_subset, 'ko-', label='Target Spectrum', markersize=5, linewidth=1.2)\n",
    "        highlight_contiguous_regions(ax1, wavelengths_subset, nan_indices_subset, 'red', 0.2, 'Missing Region')\n",
    "        highlight_contiguous_regions(ax1, wavelengths_subset, observed_indices_subset, 'green', 0.2, 'Observed Region')\n",
    "        ax1.set_xlabel(\"Wavelength (µm)\", fontsize=24)\n",
    "        ax1.set_ylabel(\"Reflectance\", fontsize=24)\n",
    "        ax1.legend(fontsize=19)\n",
    "        ax1.tick_params(axis='both', which='major', labelsize=19)\n",
    "        ax1.set_xlim(common_xlim)\n",
    "\n",
    "        # --- Plot on the second subplot (axes[0, 1]) with left-aligned candidates (from similarity) ---\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.plot(wavelengths_subset, sample_subset, 'ko-', label='Target Spectrum', markersize=5, linewidth=1.2, zorder=10)\n",
    "        #highlight_contiguous_regions(ax2, wavelengths_subset, nan_indices_subset, 'red', 0.2, '_nolegend_')\n",
    "        highlight_contiguous_regions(ax2, wavelengths_subset, observed_indices_subset, 'green', 0.2, '_nolegend_')\n",
    "        ax2.set_xlabel(\"Wavelength (µm)\", fontsize=24)\n",
    "        ax2.tick_params(axis='both', which='major', labelsize=19)\n",
    "        ax2.yaxis.set_visible(False)  # Hide y-axis labels and ticks\n",
    "\n",
    "        # Align candidates visually based on the first observed point\n",
    "        anchor_plot_indices = observed_indices_subset[:1] # Use first observed point as anchor for plot 2\n",
    "        if anchor_plot_indices.size > 0 and top_candidates_left_info:\n",
    "            # Extract just the spectra for plotting\n",
    "            top_spectra_for_plot2 = [info[0] for info in top_candidates_left_info]\n",
    "            for candidate_spec in top_spectra_for_plot2:\n",
    "                 # Align the *entire* candidate spectrum for visualization\n",
    "                 aligned_candidate_vis = align_candidate_point(sample_subset, candidate_spec, anchor_plot_indices, side=\"left\")\n",
    "                 ax2.plot(wavelengths_subset, aligned_candidate_vis, color='gray', linewidth=0.7, alpha=0.6, zorder=1)\n",
    "        ax2.set_xlim(common_xlim)\n",
    "\n",
    "        # --- Plot on the third subplot (axes[1, 0]) with MSE-aligned candidates ---\n",
    "        ax3 = axes[1, 0]\n",
    "        ax3.plot(wavelengths_subset, sample_subset, 'ko-', label='Target Spectrum', markersize=5, linewidth=1.2, zorder=10)\n",
    "        #highlight_contiguous_regions(ax3, wavelengths_subset, nan_indices_subset, 'red', 0.2, '_nolegend_')\n",
    "        highlight_contiguous_regions(ax3, wavelengths_subset, observed_indices_subset, 'green', 0.2, '_nolegend_')\n",
    "        ax3.set_xlabel(\"Wavelength (µm)\", fontsize=24)\n",
    "        ax3.set_ylabel(\"Reflectance\", fontsize=24) # Show Y label here again\n",
    "        ax3.tick_params(axis='both', which='major', labelsize=19)\n",
    "\n",
    "        if top_candidates_mse_aligned_spectra and observed_indices_subset.size > 0:\n",
    "            target_obs_part_for_mse = sample_subset[observed_indices_subset]\n",
    "            for candidate_spec in top_candidates_mse_aligned_spectra:\n",
    "                candidate_obs_part = candidate_spec[observed_indices_subset]\n",
    "                # Calculate the optimal shift based on MSE over the observed part\n",
    "                best_shift_mse = align_candidate_mse(target_obs_part_for_mse, candidate_obs_part, np.arange(len(target_obs_part_for_mse))) # align_candidate_mse returns shift now? No, returns aligned part. Let's recalculate shift\n",
    "                valid_mse = ~np.isnan(target_obs_part_for_mse) & ~np.isnan(candidate_obs_part)\n",
    "                if np.sum(valid_mse) > 0:\n",
    "                     best_shift_mse_val = np.mean(target_obs_part_for_mse[valid_mse]) - np.mean(candidate_obs_part[valid_mse])\n",
    "                else: best_shift_mse_val = 0\n",
    "\n",
    "                # Apply shift to the *entire* candidate spectrum for plotting\n",
    "                aligned_candidate_mse_plot = candidate_spec + best_shift_mse_val\n",
    "                ax3.plot(wavelengths_subset, aligned_candidate_mse_plot, color='grey', linewidth=0.7, alpha=0.8, zorder=1) # Use lightblue for MSE plot\n",
    "        ax3.set_xlim(common_xlim)\n",
    "\n",
    "        # ********************************************************************\n",
    "        # --- Calculate Imputed Data & Plot on Fourth Subplot (axes[1, 1]) ---\n",
    "        # ********************************************************************\n",
    "        imputed_values_plot = np.array([]) # Initialize empty array for imputed points\n",
    "        left_missing_indices_subset = np.array([], dtype=int) # Initialize\n",
    "\n",
    "        # Use candidates found by similarity metric (top_candidates_left_info)\n",
    "        # Need candidates with errors, observed points, and missing points to the left\n",
    "        if top_candidates_left_info and observed_indices_subset.size > 0:\n",
    "            # Identify indices within the 'subset' that are NaN AND before the first observed point\n",
    "            first_observed_idx_in_subset = observed_indices_subset[0]\n",
    "            left_missing_indices_subset = nan_indices_subset[nan_indices_subset < first_observed_idx_in_subset]\n",
    "\n",
    "            # Define overlap indices for alignment/slope (use first few observed points)\n",
    "            # Need at least 2 points for slope calculation for the smoothing step\n",
    "            num_overlap_points_impute = min(5, len(observed_indices_subset)) # Use up to 5 points\n",
    "            left_overlap_indices_subset = observed_indices_subset[:num_overlap_points_impute]\n",
    "\n",
    "            if left_missing_indices_subset.size > 0 and left_overlap_indices_subset.size > 0:\n",
    "                print(f\"Target {target_index}: Calculating imputation for {len(left_missing_indices_subset)} left missing points.\")\n",
    "\n",
    "                # Extract candidate spectra and errors from the list of tuples\n",
    "                imputed_left_candidates_spectra = [info[0] for info in top_candidates_left_info]\n",
    "                errors_left = np.array([info[1] for info in top_candidates_left_info])\n",
    "\n",
    "                imputed_left_values_collected = [] # To store the relevant part from each *aligned* candidate\n",
    "\n",
    "                # Use the first overlap point as the reference for alignment shift for imputation\n",
    "                anchor_idx_impute_subset = left_overlap_indices_subset[0]\n",
    "\n",
    "                for cand_spec_subset in imputed_left_candidates_spectra:\n",
    "                    # Align the candidate based on the anchor point\n",
    "                    # Ensure anchor point is valid before aligning\n",
    "                    shift_point_impute = 0\n",
    "                    if anchor_idx_impute_subset < len(sample_subset) and anchor_idx_impute_subset < len(cand_spec_subset) and \\\n",
    "                       not np.isnan(sample_subset[anchor_idx_impute_subset]) and not np.isnan(cand_spec_subset[anchor_idx_impute_subset]):\n",
    "                         shift_point_impute = sample_subset[anchor_idx_impute_subset] - cand_spec_subset[anchor_idx_impute_subset]\n",
    "                    else:\n",
    "                        # Handle case where anchor point itself is NaN - maybe skip this candidate or use mean shift?\n",
    "                        # For now, let's assume a zero shift if anchor is bad, though this might be suboptimal\n",
    "                        print(f\"Warning: Anchor point {anchor_idx_impute_subset} invalid for alignment in imputation. Using shift=0 for this candidate.\")\n",
    "\n",
    "                    cand_aligned = cand_spec_subset + shift_point_impute\n",
    "                    # Extract the values corresponding to the *left missing* indices from the aligned candidate\n",
    "                    imputed_left_values_collected.append(cand_aligned[left_missing_indices_subset])\n",
    "\n",
    "                if imputed_left_values_collected: # Ensure we collected some values\n",
    "                    imputed_left_values_collected = np.array(imputed_left_values_collected)\n",
    "\n",
    "                    # --- Weighted Average ---\n",
    "                    weights_left = 1.0 / (errors_left + 1e-9) # Add epsilon for stability\n",
    "                    # Check if weights are valid before averaging\n",
    "                    if np.sum(weights_left) > 0 and np.all(np.isfinite(weights_left)):\n",
    "                        weighted_imputed_left = np.average(imputed_left_values_collected, axis=0, weights=weights_left)\n",
    "                    else:\n",
    "                         print(f\"Target {target_index}: Warning - Invalid weights for imputation. Using simple mean.\")\n",
    "                         # Ensure errors_left and imputed_left_values_collected match in number of candidates\n",
    "                         if len(errors_left) == imputed_left_values_collected.shape[0]:\n",
    "                             weighted_imputed_left = np.mean(imputed_left_values_collected, axis=0)\n",
    "                         else:\n",
    "                              print(f\"Target {target_index}: Error - Mismatch in number of errors and collected values. Cannot compute mean.\")\n",
    "                              weighted_imputed_left = np.full(left_missing_indices_subset.shape, np.nan) # Fallback\n",
    "\n",
    "\n",
    "                    # --- Smoothing/Extrapolation based on slope (optional, based on snippet) ---\n",
    "                    if left_overlap_indices_subset.size >= 2:\n",
    "                        # Calculate slope from the first two observed points of the target\n",
    "                        idx0 = left_overlap_indices_subset[0]\n",
    "                        idx1 = left_overlap_indices_subset[1]\n",
    "                        x0_wl = wavelengths_subset[idx0]\n",
    "                        x1_wl = wavelengths_subset[idx1]\n",
    "                        y0_val = sample_subset[idx0]\n",
    "                        y1_val = sample_subset[idx1]\n",
    "                        # Avoid division by zero if wavelengths are identical\n",
    "                        slope_left = (y1_val - y0_val) / (x1_wl - x0_wl) if (x1_wl - x0_wl) != 0 else 0\n",
    "                    else:\n",
    "                        slope_left = 0 # Cannot calculate slope with < 2 points\n",
    "\n",
    "                    # Apply smoothing using distance-based weights\n",
    "                    smoothed_left = weighted_imputed_left.copy()\n",
    "                    first_obs_idx_for_smoothing = observed_indices_subset[0] # Index within the subset\n",
    "\n",
    "                    for idx_in_smoothed_array, missing_idx_in_subset in enumerate(left_missing_indices_subset):\n",
    "                        d = first_obs_idx_for_smoothing - missing_idx_in_subset # Distance in indices\n",
    "\n",
    "                        # Define weights based on distance 'd' (same as snippet)\n",
    "                        if d == 1: w = 0.5\n",
    "                        elif d == 2: w = 0.35\n",
    "                        elif d == 3: w = 0.25\n",
    "                        elif d == 4: w = 0.15\n",
    "                        elif d == 5: w = 0.10\n",
    "                        elif d == 6: w = 0.05\n",
    "                        else: w = 0.0 # No smoothing for points far away\n",
    "\n",
    "                        if w > 0 and slope_left != 0 and np.isfinite(slope_left):\n",
    "                            # Extrapolate from the first observed point\n",
    "                            missing_wl = wavelengths_subset[missing_idx_in_subset]\n",
    "                            x0_wl_smooth = wavelengths_subset[first_obs_idx_for_smoothing] # Wavelength of first observed\n",
    "                            y0_val_smooth = sample_subset[first_obs_idx_for_smoothing] # Value of first observed\n",
    "\n",
    "                            # Ensure y0_val_smooth is valid before extrapolating\n",
    "                            if not np.isnan(y0_val_smooth):\n",
    "                                 extrapolated_val = y0_val_smooth - slope_left * (x0_wl_smooth - missing_wl) # y = y0 + m*(x-x0) -> y = y0 - m*(x0-x)\n",
    "                                 # Combine weighted average with extrapolation, check if current value is valid\n",
    "                                 current_weighted_val = weighted_imputed_left[idx_in_smoothed_array]\n",
    "                                 if np.isfinite(extrapolated_val) and np.isfinite(current_weighted_val):\n",
    "                                      smoothed_left[idx_in_smoothed_array] = w * extrapolated_val + (1 - w) * current_weighted_val\n",
    "                                 elif np.isfinite(extrapolated_val): # If only extrapolation is valid, use it partially? Or just keep original?\n",
    "                                     # Maybe lean towards extrapolation if weighted avg failed? Or stick to weighted if valid?\n",
    "                                     # Let's stick to weighted if valid, otherwise try extrapolation biased?\n",
    "                                     # Safest: only combine if both are valid.\n",
    "                                     pass # Keep original weighted if extrapolation or weighted is NaN/inf\n",
    "                            # else: Cannot extrapolate if the anchor point is NaN\n",
    "\n",
    "                        # else: keep the original weighted_imputed_left[idx_in_smoothed_array] value if w=0 or slope invalid\n",
    "\n",
    "                    # Final imputed values for plotting are the smoothed ones\n",
    "                    imputed_values_plot = smoothed_left\n",
    "                    print(f\"Target {target_index}: Imputation & Smoothing successful.\")\n",
    "                else:\n",
    "                     print(f\"Target {target_index}: Imputation failed: No candidate values were collected after alignment.\")\n",
    "            else:\n",
    "                if left_missing_indices_subset.size == 0:\n",
    "                    print(f\"Target {target_index}: Skipping imputation: No missing values found to the left of observed data.\")\n",
    "                elif left_overlap_indices_subset.size == 0:\n",
    "                    print(f\"Target {target_index}: Skipping imputation: No overlap points found.\")\n",
    "                # Handle case where top_candidates_left_info was empty\n",
    "                elif not top_candidates_left_info:\n",
    "                     print(f\"Target {target_index}: Skipping imputation: No similarity candidates found.\")\n",
    "\n",
    "\n",
    "        # --- Plot on the fourth subplot (axes[1, 1]) ---\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.plot(wavelengths_subset, sample_subset, 'ko-', label='Target Spectrum', markersize=5, zorder=10, linewidth=1.2) # Reduced marker size\n",
    "        # Highlight regions again for context\n",
    "        highlight_contiguous_regions(ax4, wavelengths_subset, nan_indices_subset, 'red', 0.2, 'Missing Region')\n",
    "        #highlight_contiguous_regions(ax4, wavelengths_subset, observed_indices_subset, 'green', 0.2, 'Observed Region')\n",
    "\n",
    "        # Plot the imputed points if they exist and match the number of missing indices\n",
    "        if imputed_values_plot.size > 0 and left_missing_indices_subset.size == imputed_values_plot.size :\n",
    "            ax4.plot(wavelengths_subset[left_missing_indices_subset], imputed_values_plot,\n",
    "                     'ro', # Red dots ('o')\n",
    "                     label='Imputed Data',\n",
    "                     markersize=6, # Adjust marker size as needed\n",
    "                     zorder=20) # Ensure points are plotted on top\n",
    "        elif imputed_values_plot.size > 0:\n",
    "             print(f\"Target {target_index}: Warning - Mismatch between number of imputed points ({imputed_values_plot.size}) and missing indices ({left_missing_indices_subset.size}). Skipping plot.\")\n",
    "\n",
    "\n",
    "        ax4.set_xlabel(\"Wavelength (µm)\", fontsize=24)\n",
    "        ax4.tick_params(axis='x', labelsize=19)\n",
    "        ax4.yaxis.set_visible(False)  # Hide y-axis labels and ticks\n",
    "        ax4.legend(fontsize=19) # Added legend\n",
    "        ax4.set_xlim(common_xlim) # Use common xlim\n",
    "\n",
    "        # Align candidates visually based on the first observed point\n",
    "        anchor_plot_indices = observed_indices_subset[:1] # Use first observed point as anchor for plot 2\n",
    "        if anchor_plot_indices.size > 0 and top_candidates_left_info:\n",
    "            # Extract just the spectra for plotting\n",
    "            top_spectra_for_plot2 = [info[0] for info in top_candidates_left_info]\n",
    "            for candidate_spec in top_spectra_for_plot2:\n",
    "                 # Align the *entire* candidate spectrum for visualization\n",
    "                 aligned_candidate_vis = align_candidate_point(sample_subset, candidate_spec, anchor_plot_indices, side=\"left\")\n",
    "                 ax4.plot(wavelengths_subset, aligned_candidate_vis, color='gray', linewidth=0.7, alpha=0.6, zorder=1)\n",
    "        ax4.set_xlim(common_xlim)\n",
    "\n",
    "        \n",
    "        # --- Final Figure Adjustments ---\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout slightly\n",
    "        plt.show()\n",
    "\n",
    "    # --- End of Q class check ---\n",
    "    elif target_class != 'Q':\n",
    "        print(f\"Skipping index {target_index}: Class is '{target_class}', not 'Q'.\")\n",
    "    else: # Class is Q, but no NaNs in the subset\n",
    "        print(f\"Skipping index {target_index}: Class 'Q' spectrum has no missing values in the 0.45-2.45 µm range.\")\n",
    "\n",
    "# --- End of loop ---\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c10193-5960-4436-ab21-820ad2ae71c7",
   "metadata": {},
   "source": [
    "## Final method - Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fed38a51-a8ba-4f57-9f27-aded4b6f07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'pV' values have been filled with mean values, and the updated CSV file '03-Base-imputedNew3.csv' has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Identify missing pV values\n",
    "missing_pV_data = df[df['pV'].isna()]\n",
    "names_missing_pV = missing_pV_data['name'].unique()\n",
    "\n",
    "# Fill missing values with mean pV of the same name\n",
    "for name in names_missing_pV:\n",
    "#    print(f\"Name: {name} (Missing pV data)\")\n",
    "    all_entries_for_name = df[df['name'] == name]\n",
    "    available_pV_data = all_entries_for_name[all_entries_for_name['pV'].notna()]\n",
    "    \n",
    "    if not available_pV_data.empty:\n",
    "        mean_pV = available_pV_data['pV'].mean()\n",
    "        df.loc[(df['name'] == name) & (df['pV'].isna()), 'pV'] = mean_pV\n",
    "#        print(f\"Filled missing 'pV' for {name} with mean value: {mean_pV:.4f}\")\n",
    "#    else:\n",
    "#        print(\"No entries with available pV data.\")\n",
    "#    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Save the updated DataFrame with the same original columns\n",
    "df.to_csv('03-Base-imputedNew3.csv', index=False)\n",
    "print(\"Missing 'pV' values have been filled with mean values, and the updated CSV file '03-Base-imputedNew3.csv' has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ad4336ac-b229-4a87-875a-dd81d5fdb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining missing 'pV' values: 551\n"
     ]
    }
   ],
   "source": [
    "# Load the updated CSV and count remaining missing pV values\n",
    "updated_df = pd.read_csv('03-Base-imputedNew3.csv')\n",
    "remaining_missing_pV = updated_df['pV'].isna().sum()\n",
    "print(f\"Total remaining missing 'pV' values: {remaining_missing_pV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b69f2723-bd4d-46ab-9986-8023fa59eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining missing 'pV' values: 551\n",
      "The PDF file with results and plots has been saved as 'pV_class_analysis.pdf'.\n",
      "Update complete. Missing 'pV' values have been imputed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Set global font to DejaVu Serif and increase font sizes\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Load CSV\n",
    "csv_file_path = '03-Base-imputedNew3.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Identify remaining missing pV values\n",
    "remaining_missing_pV_data = df[df['pV'].isna()]\n",
    "remaining_missing_pV_count = remaining_missing_pV_data.shape[0]\n",
    "print(f\"Total remaining missing 'pV' values: {remaining_missing_pV_count}\")\n",
    "\n",
    "# Fill missing pV values based on class\n",
    "pdf_filename = 'pV_class_analysis.pdf'\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    for classi in df['class_asteroid_sf'].unique():\n",
    "        class_samples = df[df['class_asteroid_sf'] == classi]\n",
    "        pV_values_class = class_samples['pV'].dropna()\n",
    "        \n",
    "        if not pV_values_class.empty:\n",
    "            Q1 = pV_values_class.quantile(0.25)\n",
    "            Q2 = pV_values_class.quantile(0.50)  # Median (Q2)\n",
    "            Q3 = pV_values_class.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            is_outlier = (pV_values_class < lower_bound) | (pV_values_class > upper_bound)\n",
    "            weights = np.where(is_outlier, 0.1, 1.0)\n",
    "            weighted_mean_pV_class = np.average(pV_values_class, weights=weights)\n",
    "            \n",
    "            missing_pV_indices = (df['class_asteroid_sf'] == classi) & (df['pV'].isna())\n",
    "            df.loc[missing_pV_indices, 'pV'] = weighted_mean_pV_class\n",
    "            \n",
    "            # Generate plot\n",
    "            plt.figure(figsize=(10, 4))  # Decreased height to shrink the plot on y-axis\n",
    "            box = plt.boxplot(pV_values_class, vert=False, patch_artist=True, showfliers=True, \n",
    "                              flierprops=dict(marker='o', color='red', alpha=0.5), widths=0.3)  # Further shrinking boxplot height\n",
    "            \n",
    "            # Increase thickness of Q2 (median) line\n",
    "            for median in box['medians']:\n",
    "                median.set(linewidth=3.5, color='orange')\n",
    "            \n",
    "            plt.axvline(x=lower_bound, color='blue', linestyle='--', label=f'Lower bound (Outlier): {lower_bound:.2f}', linewidth=3)\n",
    "            plt.axvline(x=upper_bound, color='blue', linestyle='--', label=f'Upper bound (Outlier): {upper_bound:.2f}', linewidth=3)\n",
    "            plt.title(f'Box plot of \"pV\" for class {classi} with outliers')\n",
    "            plt.xlabel('log10 pV')\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.tight_layout(rect=[0, 0.02, 1, 1])  # Adjust layout to move content up\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "\n",
    "print(f\"The PDF file with results and plots has been saved as '{pdf_filename}'.\")\n",
    "\n",
    "# Save the updated DataFrame with filled pV values\n",
    "df.to_csv('03-Base-imputedNew223.csv', index=False)\n",
    "print(\"Update complete. Missing 'pV' values have been imputed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4819f-c8d0-47e2-8114-c17185e89565",
   "metadata": {},
   "source": [
    "## Final database (concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "05cc4195-54f4-46e5-9393-dbc38b8491a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete. The new file '05-BaseNew.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('03-Base-imputedNew.csv')  # Original spectra dataset\n",
    "df2 = pd.read_csv('03-Base-imputedNew223.csv')  # Contains extra columns\n",
    "extra_columns = ['pV', 'name', 'counts', 'class_bdm', 'class_asteroid_sf']\n",
    "df2_extra = df2[extra_columns]\n",
    "# Ensure both datasets have the same length before concatenation\n",
    "if len(df1) == len(df2_extra):\n",
    "    merged_df = pd.concat([df1, df2_extra], axis=1)  # Concatenate columns\n",
    "    merged_df.to_csv('05-BaseNew.csv', index=False)\n",
    "    print(\"Merging complete. The new file '05-BaseNew.csv' has been created.\")\n",
    "else:\n",
    "    print(\"Error: The two datasets have different numbers of rows. Check for missing or extra data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6dbb4ccb-0849-4b16-a62c-0a9189ce29a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.475</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.525</th>\n",
       "      <th>0.55</th>\n",
       "      <th>0.575</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.625</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.675</th>\n",
       "      <th>...</th>\n",
       "      <th>2.25</th>\n",
       "      <th>2.3</th>\n",
       "      <th>2.35</th>\n",
       "      <th>2.4</th>\n",
       "      <th>2.45</th>\n",
       "      <th>pV</th>\n",
       "      <th>name</th>\n",
       "      <th>counts</th>\n",
       "      <th>class_bdm</th>\n",
       "      <th>class_asteroid_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474660</td>\n",
       "      <td>0.510748</td>\n",
       "      <td>0.547428</td>\n",
       "      <td>0.588601</td>\n",
       "      <td>0.620076</td>\n",
       "      <td>0.656002</td>\n",
       "      <td>0.683170</td>\n",
       "      <td>0.709885</td>\n",
       "      <td>0.741209</td>\n",
       "      <td>0.772798</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195193</td>\n",
       "      <td>1.196479</td>\n",
       "      <td>1.194755</td>\n",
       "      <td>1.189291</td>\n",
       "      <td>1.183298</td>\n",
       "      <td>0.25645</td>\n",
       "      <td>1988 TA</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.405455</td>\n",
       "      <td>0.433723</td>\n",
       "      <td>0.469353</td>\n",
       "      <td>0.504947</td>\n",
       "      <td>0.536460</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>0.593231</td>\n",
       "      <td>0.622140</td>\n",
       "      <td>0.650964</td>\n",
       "      <td>0.675510</td>\n",
       "      <td>...</td>\n",
       "      <td>1.260412</td>\n",
       "      <td>1.261258</td>\n",
       "      <td>1.244087</td>\n",
       "      <td>1.238516</td>\n",
       "      <td>1.238515</td>\n",
       "      <td>0.40600</td>\n",
       "      <td>1990 WO3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.317047</td>\n",
       "      <td>0.347344</td>\n",
       "      <td>0.384102</td>\n",
       "      <td>0.419826</td>\n",
       "      <td>0.452776</td>\n",
       "      <td>0.484209</td>\n",
       "      <td>0.509688</td>\n",
       "      <td>0.536324</td>\n",
       "      <td>0.569470</td>\n",
       "      <td>0.597419</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246545</td>\n",
       "      <td>1.253701</td>\n",
       "      <td>1.248591</td>\n",
       "      <td>1.248602</td>\n",
       "      <td>1.271996</td>\n",
       "      <td>0.25645</td>\n",
       "      <td>1993 FO6</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.479208</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.554585</td>\n",
       "      <td>0.594870</td>\n",
       "      <td>0.628223</td>\n",
       "      <td>0.663880</td>\n",
       "      <td>0.689781</td>\n",
       "      <td>0.716422</td>\n",
       "      <td>0.756680</td>\n",
       "      <td>0.787789</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167623</td>\n",
       "      <td>1.163372</td>\n",
       "      <td>1.155753</td>\n",
       "      <td>1.148710</td>\n",
       "      <td>1.146478</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>1998 FE118</td>\n",
       "      <td>1</td>\n",
       "      <td>Sa</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.399207</td>\n",
       "      <td>0.429273</td>\n",
       "      <td>0.465856</td>\n",
       "      <td>0.502021</td>\n",
       "      <td>0.535473</td>\n",
       "      <td>0.567403</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>0.619699</td>\n",
       "      <td>0.649162</td>\n",
       "      <td>0.676829</td>\n",
       "      <td>...</td>\n",
       "      <td>1.234882</td>\n",
       "      <td>1.222925</td>\n",
       "      <td>1.222321</td>\n",
       "      <td>1.224672</td>\n",
       "      <td>1.228380</td>\n",
       "      <td>0.37800</td>\n",
       "      <td>1998 YG8</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.449156</td>\n",
       "      <td>0.464809</td>\n",
       "      <td>0.479891</td>\n",
       "      <td>0.489143</td>\n",
       "      <td>0.503325</td>\n",
       "      <td>0.517542</td>\n",
       "      <td>0.531750</td>\n",
       "      <td>0.545955</td>\n",
       "      <td>0.560139</td>\n",
       "      <td>...</td>\n",
       "      <td>1.378472</td>\n",
       "      <td>1.401883</td>\n",
       "      <td>1.418678</td>\n",
       "      <td>1.437153</td>\n",
       "      <td>1.456324</td>\n",
       "      <td>0.22000</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>0.467633</td>\n",
       "      <td>0.480760</td>\n",
       "      <td>0.494760</td>\n",
       "      <td>0.508886</td>\n",
       "      <td>0.516543</td>\n",
       "      <td>0.529372</td>\n",
       "      <td>0.542561</td>\n",
       "      <td>0.555911</td>\n",
       "      <td>0.570074</td>\n",
       "      <td>0.584662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322165</td>\n",
       "      <td>1.346740</td>\n",
       "      <td>1.371653</td>\n",
       "      <td>1.394933</td>\n",
       "      <td>1.413759</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>0.424824</td>\n",
       "      <td>0.438244</td>\n",
       "      <td>0.452745</td>\n",
       "      <td>0.467561</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>0.497040</td>\n",
       "      <td>0.508614</td>\n",
       "      <td>0.521423</td>\n",
       "      <td>0.536404</td>\n",
       "      <td>0.551251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.242833</td>\n",
       "      <td>1.252417</td>\n",
       "      <td>1.266928</td>\n",
       "      <td>1.286292</td>\n",
       "      <td>1.317274</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>Thestor</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>0.654094</td>\n",
       "      <td>0.649106</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.649133</td>\n",
       "      <td>0.649091</td>\n",
       "      <td>0.652669</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>0.661425</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.674114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.272509</td>\n",
       "      <td>1.285464</td>\n",
       "      <td>1.299044</td>\n",
       "      <td>1.310190</td>\n",
       "      <td>1.315834</td>\n",
       "      <td>0.04400</td>\n",
       "      <td>Thule</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>0.520102</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.554007</td>\n",
       "      <td>0.567428</td>\n",
       "      <td>0.579296</td>\n",
       "      <td>0.590139</td>\n",
       "      <td>0.601631</td>\n",
       "      <td>0.612858</td>\n",
       "      <td>0.624339</td>\n",
       "      <td>0.636803</td>\n",
       "      <td>...</td>\n",
       "      <td>1.336246</td>\n",
       "      <td>1.353193</td>\n",
       "      <td>1.370389</td>\n",
       "      <td>1.388239</td>\n",
       "      <td>1.401814</td>\n",
       "      <td>0.05500</td>\n",
       "      <td>Tornio</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2940 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.45     0.475       0.5     0.525      0.55     0.575       0.6  \\\n",
       "0     0.474660  0.510748  0.547428  0.588601  0.620076  0.656002  0.683170   \n",
       "1     0.405455  0.433723  0.469353  0.504947  0.536460  0.567352  0.593231   \n",
       "2     0.317047  0.347344  0.384102  0.419826  0.452776  0.484209  0.509688   \n",
       "3     0.479208  0.514880  0.554585  0.594870  0.628223  0.663880  0.689781   \n",
       "4     0.399207  0.429273  0.465856  0.502021  0.535473  0.567403  0.593125   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2935  0.433300  0.449156  0.464809  0.479891  0.489143  0.503325  0.517542   \n",
       "2936  0.467633  0.480760  0.494760  0.508886  0.516543  0.529372  0.542561   \n",
       "2937  0.424824  0.438244  0.452745  0.467561  0.482917  0.497040  0.508614   \n",
       "2938  0.654094  0.649106  0.647750  0.649133  0.649091  0.652669  0.654123   \n",
       "2939  0.520102  0.538432  0.554007  0.567428  0.579296  0.590139  0.601631   \n",
       "\n",
       "         0.625      0.65     0.675  ...      2.25       2.3      2.35  \\\n",
       "0     0.709885  0.741209  0.772798  ...  1.195193  1.196479  1.194755   \n",
       "1     0.622140  0.650964  0.675510  ...  1.260412  1.261258  1.244087   \n",
       "2     0.536324  0.569470  0.597419  ...  1.246545  1.253701  1.248591   \n",
       "3     0.716422  0.756680  0.787789  ...  1.167623  1.163372  1.155753   \n",
       "4     0.619699  0.649162  0.676829  ...  1.234882  1.222925  1.222321   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2935  0.531750  0.545955  0.560139  ...  1.378472  1.401883  1.418678   \n",
       "2936  0.555911  0.570074  0.584662  ...  1.322165  1.346740  1.371653   \n",
       "2937  0.521423  0.536404  0.551251  ...  1.242833  1.252417  1.266928   \n",
       "2938  0.661425  0.668186  0.674114  ...  1.272509  1.285464  1.299044   \n",
       "2939  0.612858  0.624339  0.636803  ...  1.336246  1.353193  1.370389   \n",
       "\n",
       "           2.4      2.45       pV        name  counts  class_bdm  \\\n",
       "0     1.189291  1.183298  0.25645     1988 TA       1          A   \n",
       "1     1.238516  1.238515  0.40600    1990 WO3       1          A   \n",
       "2     1.248602  1.271996  0.25645    1993 FO6       1          A   \n",
       "3     1.148710  1.146478  0.22100  1998 FE118       1         Sa   \n",
       "4     1.224672  1.228380  0.37800    1998 YG8       1          A   \n",
       "...        ...       ...      ...         ...     ...        ...   \n",
       "2935  1.437153  1.456324  0.22000     Skorina       3          D   \n",
       "2936  1.394933  1.413759  0.11000     Skorina       1          D   \n",
       "2937  1.286292  1.317274  0.06700     Thestor       1          D   \n",
       "2938  1.310190  1.315834  0.04400       Thule       1          D   \n",
       "2939  1.388239  1.401814  0.05500      Tornio       1          D   \n",
       "\n",
       "      class_asteroid_sf  \n",
       "0                     A  \n",
       "1                     A  \n",
       "2                     A  \n",
       "3                     A  \n",
       "4                     A  \n",
       "...                 ...  \n",
       "2935                  Z  \n",
       "2936                  Z  \n",
       "2937                  Z  \n",
       "2938                  Z  \n",
       "2939                  Z  \n",
       "\n",
       "[2940 rows x 58 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "64d299d8-5105-408f-9a0d-54d7b7ab84ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalized to 0.55 micrometers and saved to '05-BaseNew1.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('05-BaseNew.csv')\n",
    "\n",
    "# Identify the column for 0.55 micrometers (5th column, index 4)\n",
    "reference_column_index = 4\n",
    "\n",
    "# Select the columns to normalize (all columns except the last 5)\n",
    "columns_to_normalize = df.columns[:-5]\n",
    "\n",
    "# Create a new DataFrame for the normalized data\n",
    "df_normalized = df[columns_to_normalize].copy()\n",
    "\n",
    "# Get the reflectance value at 0.55 micrometers for each sample\n",
    "reference_reflectance = df.iloc[:, reference_column_index]\n",
    "\n",
    "# Normalize each column by the reference reflectance\n",
    "for col in columns_to_normalize:\n",
    "    df_normalized[col] = df_normalized[col] / reference_reflectance\n",
    "\n",
    "# Add back any columns that were excluded (the last 5)\n",
    "df_normalized = pd.concat([df_normalized, df[df.columns[-5:]]], axis=1)\n",
    "\n",
    "# Export the normalized DataFrame to a new CSV file\n",
    "df_normalized.to_csv('05-BaseNew1.csv', index=False)\n",
    "\n",
    "print(\"Data normalized to 0.55 micrometers and saved to '05-BaseNew1.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1f6a42ac-3eb9-4f28-b3c0-6d3917c8f0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.475</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.525</th>\n",
       "      <th>0.55</th>\n",
       "      <th>0.575</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.625</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.675</th>\n",
       "      <th>...</th>\n",
       "      <th>2.25</th>\n",
       "      <th>2.3</th>\n",
       "      <th>2.35</th>\n",
       "      <th>2.4</th>\n",
       "      <th>2.45</th>\n",
       "      <th>pV</th>\n",
       "      <th>name</th>\n",
       "      <th>counts</th>\n",
       "      <th>class_bdm</th>\n",
       "      <th>class_asteroid_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765487</td>\n",
       "      <td>0.823687</td>\n",
       "      <td>0.882840</td>\n",
       "      <td>0.949241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.057939</td>\n",
       "      <td>1.101752</td>\n",
       "      <td>1.144836</td>\n",
       "      <td>1.195353</td>\n",
       "      <td>1.246295</td>\n",
       "      <td>...</td>\n",
       "      <td>1.927495</td>\n",
       "      <td>1.929570</td>\n",
       "      <td>1.926789</td>\n",
       "      <td>1.917978</td>\n",
       "      <td>1.908312</td>\n",
       "      <td>0.25645</td>\n",
       "      <td>1988 TA</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.755798</td>\n",
       "      <td>0.808492</td>\n",
       "      <td>0.874908</td>\n",
       "      <td>0.941258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.057586</td>\n",
       "      <td>1.105827</td>\n",
       "      <td>1.159715</td>\n",
       "      <td>1.213445</td>\n",
       "      <td>1.259200</td>\n",
       "      <td>...</td>\n",
       "      <td>2.349500</td>\n",
       "      <td>2.351078</td>\n",
       "      <td>2.319069</td>\n",
       "      <td>2.308685</td>\n",
       "      <td>2.308683</td>\n",
       "      <td>0.40600</td>\n",
       "      <td>1990 WO3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700229</td>\n",
       "      <td>0.767144</td>\n",
       "      <td>0.848328</td>\n",
       "      <td>0.927227</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.069424</td>\n",
       "      <td>1.125698</td>\n",
       "      <td>1.184525</td>\n",
       "      <td>1.257731</td>\n",
       "      <td>1.319459</td>\n",
       "      <td>...</td>\n",
       "      <td>2.753119</td>\n",
       "      <td>2.768924</td>\n",
       "      <td>2.757638</td>\n",
       "      <td>2.757661</td>\n",
       "      <td>2.809330</td>\n",
       "      <td>0.25645</td>\n",
       "      <td>1993 FO6</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.762798</td>\n",
       "      <td>0.819581</td>\n",
       "      <td>0.882783</td>\n",
       "      <td>0.946908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.056758</td>\n",
       "      <td>1.097987</td>\n",
       "      <td>1.140394</td>\n",
       "      <td>1.204475</td>\n",
       "      <td>1.253995</td>\n",
       "      <td>...</td>\n",
       "      <td>1.858611</td>\n",
       "      <td>1.851845</td>\n",
       "      <td>1.839717</td>\n",
       "      <td>1.828506</td>\n",
       "      <td>1.824952</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>1998 FE118</td>\n",
       "      <td>1</td>\n",
       "      <td>Sa</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.745522</td>\n",
       "      <td>0.801671</td>\n",
       "      <td>0.869989</td>\n",
       "      <td>0.937528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.059629</td>\n",
       "      <td>1.107666</td>\n",
       "      <td>1.157293</td>\n",
       "      <td>1.212315</td>\n",
       "      <td>1.263984</td>\n",
       "      <td>...</td>\n",
       "      <td>2.306153</td>\n",
       "      <td>2.283822</td>\n",
       "      <td>2.282694</td>\n",
       "      <td>2.287084</td>\n",
       "      <td>2.294010</td>\n",
       "      <td>0.37800</td>\n",
       "      <td>1998 YG8</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>0.885836</td>\n",
       "      <td>0.918252</td>\n",
       "      <td>0.950252</td>\n",
       "      <td>0.981087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.028994</td>\n",
       "      <td>1.058060</td>\n",
       "      <td>1.087105</td>\n",
       "      <td>1.116146</td>\n",
       "      <td>1.145145</td>\n",
       "      <td>...</td>\n",
       "      <td>2.818139</td>\n",
       "      <td>2.866001</td>\n",
       "      <td>2.900336</td>\n",
       "      <td>2.938107</td>\n",
       "      <td>2.977300</td>\n",
       "      <td>0.22000</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.930726</td>\n",
       "      <td>0.957829</td>\n",
       "      <td>0.985176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.024837</td>\n",
       "      <td>1.050369</td>\n",
       "      <td>1.076213</td>\n",
       "      <td>1.103633</td>\n",
       "      <td>1.131874</td>\n",
       "      <td>...</td>\n",
       "      <td>2.559641</td>\n",
       "      <td>2.607216</td>\n",
       "      <td>2.655446</td>\n",
       "      <td>2.700515</td>\n",
       "      <td>2.736961</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>0.879704</td>\n",
       "      <td>0.907492</td>\n",
       "      <td>0.937520</td>\n",
       "      <td>0.968200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.029245</td>\n",
       "      <td>1.053212</td>\n",
       "      <td>1.079734</td>\n",
       "      <td>1.110757</td>\n",
       "      <td>1.141502</td>\n",
       "      <td>...</td>\n",
       "      <td>2.573592</td>\n",
       "      <td>2.593439</td>\n",
       "      <td>2.623487</td>\n",
       "      <td>2.663585</td>\n",
       "      <td>2.727741</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>Thestor</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>1.007708</td>\n",
       "      <td>1.000023</td>\n",
       "      <td>0.997933</td>\n",
       "      <td>1.000065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.005513</td>\n",
       "      <td>1.007752</td>\n",
       "      <td>1.019002</td>\n",
       "      <td>1.029418</td>\n",
       "      <td>1.038550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.960448</td>\n",
       "      <td>1.980407</td>\n",
       "      <td>2.001328</td>\n",
       "      <td>2.018500</td>\n",
       "      <td>2.027195</td>\n",
       "      <td>0.04400</td>\n",
       "      <td>Thule</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>0.897818</td>\n",
       "      <td>0.929460</td>\n",
       "      <td>0.956346</td>\n",
       "      <td>0.979514</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.018717</td>\n",
       "      <td>1.038556</td>\n",
       "      <td>1.057935</td>\n",
       "      <td>1.077755</td>\n",
       "      <td>1.099271</td>\n",
       "      <td>...</td>\n",
       "      <td>2.306673</td>\n",
       "      <td>2.335927</td>\n",
       "      <td>2.365611</td>\n",
       "      <td>2.396425</td>\n",
       "      <td>2.419857</td>\n",
       "      <td>0.05500</td>\n",
       "      <td>Tornio</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2940 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.45     0.475       0.5     0.525  0.55     0.575       0.6  \\\n",
       "0     0.765487  0.823687  0.882840  0.949241   1.0  1.057939  1.101752   \n",
       "1     0.755798  0.808492  0.874908  0.941258   1.0  1.057586  1.105827   \n",
       "2     0.700229  0.767144  0.848328  0.927227   1.0  1.069424  1.125698   \n",
       "3     0.762798  0.819581  0.882783  0.946908   1.0  1.056758  1.097987   \n",
       "4     0.745522  0.801671  0.869989  0.937528   1.0  1.059629  1.107666   \n",
       "...        ...       ...       ...       ...   ...       ...       ...   \n",
       "2935  0.885836  0.918252  0.950252  0.981087   1.0  1.028994  1.058060   \n",
       "2936  0.905312  0.930726  0.957829  0.985176   1.0  1.024837  1.050369   \n",
       "2937  0.879704  0.907492  0.937520  0.968200   1.0  1.029245  1.053212   \n",
       "2938  1.007708  1.000023  0.997933  1.000065   1.0  1.005513  1.007752   \n",
       "2939  0.897818  0.929460  0.956346  0.979514   1.0  1.018717  1.038556   \n",
       "\n",
       "         0.625      0.65     0.675  ...      2.25       2.3      2.35  \\\n",
       "0     1.144836  1.195353  1.246295  ...  1.927495  1.929570  1.926789   \n",
       "1     1.159715  1.213445  1.259200  ...  2.349500  2.351078  2.319069   \n",
       "2     1.184525  1.257731  1.319459  ...  2.753119  2.768924  2.757638   \n",
       "3     1.140394  1.204475  1.253995  ...  1.858611  1.851845  1.839717   \n",
       "4     1.157293  1.212315  1.263984  ...  2.306153  2.283822  2.282694   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2935  1.087105  1.116146  1.145145  ...  2.818139  2.866001  2.900336   \n",
       "2936  1.076213  1.103633  1.131874  ...  2.559641  2.607216  2.655446   \n",
       "2937  1.079734  1.110757  1.141502  ...  2.573592  2.593439  2.623487   \n",
       "2938  1.019002  1.029418  1.038550  ...  1.960448  1.980407  2.001328   \n",
       "2939  1.057935  1.077755  1.099271  ...  2.306673  2.335927  2.365611   \n",
       "\n",
       "           2.4      2.45       pV        name  counts  class_bdm  \\\n",
       "0     1.917978  1.908312  0.25645     1988 TA       1          A   \n",
       "1     2.308685  2.308683  0.40600    1990 WO3       1          A   \n",
       "2     2.757661  2.809330  0.25645    1993 FO6       1          A   \n",
       "3     1.828506  1.824952  0.22100  1998 FE118       1         Sa   \n",
       "4     2.287084  2.294010  0.37800    1998 YG8       1          A   \n",
       "...        ...       ...      ...         ...     ...        ...   \n",
       "2935  2.938107  2.977300  0.22000     Skorina       3          D   \n",
       "2936  2.700515  2.736961  0.11000     Skorina       1          D   \n",
       "2937  2.663585  2.727741  0.06700     Thestor       1          D   \n",
       "2938  2.018500  2.027195  0.04400       Thule       1          D   \n",
       "2939  2.396425  2.419857  0.05500      Tornio       1          D   \n",
       "\n",
       "      class_asteroid_sf  \n",
       "0                     A  \n",
       "1                     A  \n",
       "2                     A  \n",
       "3                     A  \n",
       "4                     A  \n",
       "...                 ...  \n",
       "2935                  Z  \n",
       "2936                  Z  \n",
       "2937                  Z  \n",
       "2938                  Z  \n",
       "2939                  Z  \n",
       "\n",
       "[2940 rows x 58 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "57198429-95c4-4723-acb4-7e6ad571660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.475</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.525</th>\n",
       "      <th>0.575</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.625</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.675</th>\n",
       "      <th>0.7</th>\n",
       "      <th>...</th>\n",
       "      <th>2.25</th>\n",
       "      <th>2.3</th>\n",
       "      <th>2.35</th>\n",
       "      <th>2.4</th>\n",
       "      <th>2.45</th>\n",
       "      <th>pV</th>\n",
       "      <th>name</th>\n",
       "      <th>counts</th>\n",
       "      <th>class_bdm</th>\n",
       "      <th>class_asteroid_sf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765487</td>\n",
       "      <td>0.823687</td>\n",
       "      <td>0.882840</td>\n",
       "      <td>0.949241</td>\n",
       "      <td>1.057939</td>\n",
       "      <td>1.101752</td>\n",
       "      <td>1.144836</td>\n",
       "      <td>1.195353</td>\n",
       "      <td>1.246295</td>\n",
       "      <td>1.277057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.927495</td>\n",
       "      <td>1.929570</td>\n",
       "      <td>1.926789</td>\n",
       "      <td>1.917978</td>\n",
       "      <td>1.908312</td>\n",
       "      <td>0.25645</td>\n",
       "      <td>1988 TA</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.755798</td>\n",
       "      <td>0.808492</td>\n",
       "      <td>0.874908</td>\n",
       "      <td>0.941258</td>\n",
       "      <td>1.057586</td>\n",
       "      <td>1.105827</td>\n",
       "      <td>1.159715</td>\n",
       "      <td>1.213445</td>\n",
       "      <td>1.259200</td>\n",
       "      <td>1.289194</td>\n",
       "      <td>...</td>\n",
       "      <td>2.349500</td>\n",
       "      <td>2.351078</td>\n",
       "      <td>2.319069</td>\n",
       "      <td>2.308685</td>\n",
       "      <td>2.308683</td>\n",
       "      <td>0.40600</td>\n",
       "      <td>1990 WO3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700229</td>\n",
       "      <td>0.767144</td>\n",
       "      <td>0.848328</td>\n",
       "      <td>0.927227</td>\n",
       "      <td>1.069424</td>\n",
       "      <td>1.125698</td>\n",
       "      <td>1.184525</td>\n",
       "      <td>1.257731</td>\n",
       "      <td>1.319459</td>\n",
       "      <td>1.360667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.753119</td>\n",
       "      <td>2.768924</td>\n",
       "      <td>2.757638</td>\n",
       "      <td>2.757661</td>\n",
       "      <td>2.809330</td>\n",
       "      <td>0.25645</td>\n",
       "      <td>1993 FO6</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.762798</td>\n",
       "      <td>0.819581</td>\n",
       "      <td>0.882783</td>\n",
       "      <td>0.946908</td>\n",
       "      <td>1.056758</td>\n",
       "      <td>1.097987</td>\n",
       "      <td>1.140394</td>\n",
       "      <td>1.204475</td>\n",
       "      <td>1.253995</td>\n",
       "      <td>1.281903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.858611</td>\n",
       "      <td>1.851845</td>\n",
       "      <td>1.839717</td>\n",
       "      <td>1.828506</td>\n",
       "      <td>1.824952</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>1998 FE118</td>\n",
       "      <td>1</td>\n",
       "      <td>Sa</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.745522</td>\n",
       "      <td>0.801671</td>\n",
       "      <td>0.869989</td>\n",
       "      <td>0.937528</td>\n",
       "      <td>1.059629</td>\n",
       "      <td>1.107666</td>\n",
       "      <td>1.157293</td>\n",
       "      <td>1.212315</td>\n",
       "      <td>1.263984</td>\n",
       "      <td>1.316203</td>\n",
       "      <td>...</td>\n",
       "      <td>2.306153</td>\n",
       "      <td>2.283822</td>\n",
       "      <td>2.282694</td>\n",
       "      <td>2.287084</td>\n",
       "      <td>2.294010</td>\n",
       "      <td>0.37800</td>\n",
       "      <td>1998 YG8</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>0.885836</td>\n",
       "      <td>0.918252</td>\n",
       "      <td>0.950252</td>\n",
       "      <td>0.981087</td>\n",
       "      <td>1.028994</td>\n",
       "      <td>1.058060</td>\n",
       "      <td>1.087105</td>\n",
       "      <td>1.116146</td>\n",
       "      <td>1.145145</td>\n",
       "      <td>1.169446</td>\n",
       "      <td>...</td>\n",
       "      <td>2.818139</td>\n",
       "      <td>2.866001</td>\n",
       "      <td>2.900336</td>\n",
       "      <td>2.938107</td>\n",
       "      <td>2.977300</td>\n",
       "      <td>0.22000</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.930726</td>\n",
       "      <td>0.957829</td>\n",
       "      <td>0.985176</td>\n",
       "      <td>1.024837</td>\n",
       "      <td>1.050369</td>\n",
       "      <td>1.076213</td>\n",
       "      <td>1.103633</td>\n",
       "      <td>1.131874</td>\n",
       "      <td>1.161987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.559641</td>\n",
       "      <td>2.607216</td>\n",
       "      <td>2.655446</td>\n",
       "      <td>2.700515</td>\n",
       "      <td>2.736961</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>Skorina</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>0.879704</td>\n",
       "      <td>0.907492</td>\n",
       "      <td>0.937520</td>\n",
       "      <td>0.968200</td>\n",
       "      <td>1.029245</td>\n",
       "      <td>1.053212</td>\n",
       "      <td>1.079734</td>\n",
       "      <td>1.110757</td>\n",
       "      <td>1.141502</td>\n",
       "      <td>1.165563</td>\n",
       "      <td>...</td>\n",
       "      <td>2.573592</td>\n",
       "      <td>2.593439</td>\n",
       "      <td>2.623487</td>\n",
       "      <td>2.663585</td>\n",
       "      <td>2.727741</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>Thestor</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>1.007708</td>\n",
       "      <td>1.000023</td>\n",
       "      <td>0.997933</td>\n",
       "      <td>1.000065</td>\n",
       "      <td>1.005513</td>\n",
       "      <td>1.007752</td>\n",
       "      <td>1.019002</td>\n",
       "      <td>1.029418</td>\n",
       "      <td>1.038550</td>\n",
       "      <td>1.051106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.960448</td>\n",
       "      <td>1.980407</td>\n",
       "      <td>2.001328</td>\n",
       "      <td>2.018500</td>\n",
       "      <td>2.027195</td>\n",
       "      <td>0.04400</td>\n",
       "      <td>Thule</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>0.897818</td>\n",
       "      <td>0.929460</td>\n",
       "      <td>0.956346</td>\n",
       "      <td>0.979514</td>\n",
       "      <td>1.018717</td>\n",
       "      <td>1.038556</td>\n",
       "      <td>1.057935</td>\n",
       "      <td>1.077755</td>\n",
       "      <td>1.099271</td>\n",
       "      <td>1.118172</td>\n",
       "      <td>...</td>\n",
       "      <td>2.306673</td>\n",
       "      <td>2.335927</td>\n",
       "      <td>2.365611</td>\n",
       "      <td>2.396425</td>\n",
       "      <td>2.419857</td>\n",
       "      <td>0.05500</td>\n",
       "      <td>Tornio</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2940 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.45     0.475       0.5     0.525     0.575       0.6     0.625  \\\n",
       "0     0.765487  0.823687  0.882840  0.949241  1.057939  1.101752  1.144836   \n",
       "1     0.755798  0.808492  0.874908  0.941258  1.057586  1.105827  1.159715   \n",
       "2     0.700229  0.767144  0.848328  0.927227  1.069424  1.125698  1.184525   \n",
       "3     0.762798  0.819581  0.882783  0.946908  1.056758  1.097987  1.140394   \n",
       "4     0.745522  0.801671  0.869989  0.937528  1.059629  1.107666  1.157293   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2935  0.885836  0.918252  0.950252  0.981087  1.028994  1.058060  1.087105   \n",
       "2936  0.905312  0.930726  0.957829  0.985176  1.024837  1.050369  1.076213   \n",
       "2937  0.879704  0.907492  0.937520  0.968200  1.029245  1.053212  1.079734   \n",
       "2938  1.007708  1.000023  0.997933  1.000065  1.005513  1.007752  1.019002   \n",
       "2939  0.897818  0.929460  0.956346  0.979514  1.018717  1.038556  1.057935   \n",
       "\n",
       "          0.65     0.675       0.7  ...      2.25       2.3      2.35  \\\n",
       "0     1.195353  1.246295  1.277057  ...  1.927495  1.929570  1.926789   \n",
       "1     1.213445  1.259200  1.289194  ...  2.349500  2.351078  2.319069   \n",
       "2     1.257731  1.319459  1.360667  ...  2.753119  2.768924  2.757638   \n",
       "3     1.204475  1.253995  1.281903  ...  1.858611  1.851845  1.839717   \n",
       "4     1.212315  1.263984  1.316203  ...  2.306153  2.283822  2.282694   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2935  1.116146  1.145145  1.169446  ...  2.818139  2.866001  2.900336   \n",
       "2936  1.103633  1.131874  1.161987  ...  2.559641  2.607216  2.655446   \n",
       "2937  1.110757  1.141502  1.165563  ...  2.573592  2.593439  2.623487   \n",
       "2938  1.029418  1.038550  1.051106  ...  1.960448  1.980407  2.001328   \n",
       "2939  1.077755  1.099271  1.118172  ...  2.306673  2.335927  2.365611   \n",
       "\n",
       "           2.4      2.45       pV        name  counts  class_bdm  \\\n",
       "0     1.917978  1.908312  0.25645     1988 TA       1          A   \n",
       "1     2.308685  2.308683  0.40600    1990 WO3       1          A   \n",
       "2     2.757661  2.809330  0.25645    1993 FO6       1          A   \n",
       "3     1.828506  1.824952  0.22100  1998 FE118       1         Sa   \n",
       "4     2.287084  2.294010  0.37800    1998 YG8       1          A   \n",
       "...        ...       ...      ...         ...     ...        ...   \n",
       "2935  2.938107  2.977300  0.22000     Skorina       3          D   \n",
       "2936  2.700515  2.736961  0.11000     Skorina       1          D   \n",
       "2937  2.663585  2.727741  0.06700     Thestor       1          D   \n",
       "2938  2.018500  2.027195  0.04400       Thule       1          D   \n",
       "2939  2.396425  2.419857  0.05500      Tornio       1          D   \n",
       "\n",
       "      class_asteroid_sf  \n",
       "0                     A  \n",
       "1                     A  \n",
       "2                     A  \n",
       "3                     A  \n",
       "4                     A  \n",
       "...                 ...  \n",
       "2935                  Z  \n",
       "2936                  Z  \n",
       "2937                  Z  \n",
       "2938                  Z  \n",
       "2939                  Z  \n",
       "\n",
       "[2940 rows x 57 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('05-BaseNew1.csv')\n",
    "column_to_drop = df.columns[4]\n",
    "df.drop(columns=[column_to_drop], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6df7e-41ad-46a9-9492-a5a1d66a5cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
